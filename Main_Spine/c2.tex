% !TeX root = ../main.tex

\xchapter{生成式EIT图像重建算法理论基础}{Theory of Generative EIT Reconstruction Algorithm}

EIT 技术是一种根据场域内部电导率分布情况获取场域内部信息的成像技术。该技术由
于其非侵入、无损伤等优势，近年来得到了广泛的应用。然而，EIT 图像重建算法由于其固
有的不足而限制的其使用场景。因此，如何提高重建图像的分辨率，并加快重建的速度，是
目前 EIT 技术的瓶颈。

近年来，深度学习技术，尤其是生成式模型，取得了突破性的成果，这也为提高 EIT 图
像重建质量提供了新的研究方向。不同与计算机视觉领域的其他图像生成任务（如利用文本
生成图像、图像分类、图像识别等），EIT 技术具有完备的数学物理模型，这就为基于深度学
习的 EIT 图像重建模型提供了大量的仿真数据支持。然而，由于其性能直接影响了临床医生
对于病情的判断，因此 EIT 图像重建算法的广泛应用需要确保其效率及可行性。

本章将主要介绍 EIT 图像重建算法的数学原理，深度学习和生成式模型的常见构件及模
型结构。随后将重点对两者之间结合的可行性以及优势作出阐述，为构建基于生成式模型的
EIT 图像重建算法提供理论基础。

\xsection{EIT基本原理}{Basic Principle of EIT}

\xsubsection{EIT问题模型}{Model of EIT Problem}
在工程数学领域通常对事物内部的特征进行参数化，用模型参数$\boldsymbol{m}$来表示，它属于模型参数域$M$。
这些参数在工程实践中通常难以观测得到。
因此会选取一些与该模型参数有显著相关性的可观测参数$\boldsymbol{d}$来推断得到模型参数，
其中$\boldsymbol{d}$属于可测量参数域$D$。
则正问题可被描述为通过已知的模型参数$\boldsymbol{m}$求解可测量参数$\boldsymbol{d}$，即\cref{equation:forward}
\begin{equation}
  \label{equation:forward}
  \boldsymbol{d} = h(\boldsymbol{m})
\end{equation}
特别的，线性正问题可被描述为\cref{equation:linear_forward}
\begin{equation}
  \label{equation:linear_forward}
  \boldsymbol{d} = \boldsymbol{H}\boldsymbol{m}
\end{equation}
其中，$\boldsymbol{H}$为矩阵算子。
则逆问题则被描述为已知测量参数$\boldsymbol{d}$求解模型参数$\boldsymbol{m}$的过程。

然而，在实际的工程中，往往无法直接获得逆问题的精确解，或根本不可能获得唯一的精确解，
即不可能满足\cref{equation:error}这是由于实际问题中的每一步都存在一定的误差。
\begin{equation}
  \label{equation:error}
  h(\boldsymbol{m}) - \boldsymbol{d} = 0
\end{equation}

特别的，对于EIT而言，其成像目标是待测场内部的电导率分布。该技术通过向未知场内注入电流，同时测
量其边界电压分布，利用麦克斯韦方程组求解出其场域内部的电导率（或阻抗，注：由于电
导率和电阻抗之间为倒数关系，可相互转换。虽 EIT 名为电阻抗断层成像，但在实际 EIT 的
研究中，通常利用电导率分布作为重建图像的直接依据，故如无特殊说明，本文的成像目标
均为电导率）分布，最后根据其内部的电导率分布重建出图像。
EIT 问题的研究可分为 EIT 正问题和 EIT 逆问题。EIT 正
问题是指已知待测场内部的电导率分布及其边界电流驱动信号，求解其内部或边界的电压或
电流分布。EIT 逆问题则是指已知场域边界的测量电压以及驱动信号，求解场域内部的电导
率分布情况。
\xsubsection{EIT正问题}{Forward Problem of EIT}
EIT 问题属于电磁场分析问题，其电导率分布、注入电流以及测量电压之间可根据麦克斯
韦(Maxwell)方程组
\begin{equation}
\label{equation:Maxwell}
    \begin{aligned}
        \nabla \times \boldsymbol{H} &= J + \frac{\partial{D}}{\partial{t}} \\
        \nabla \cdot \boldsymbol{D} &= \rho \\
        \nabla \cdot \boldsymbol{B} &= 0 \\
        \nabla \times\boldsymbol{E} &= -\frac{\partial \boldsymbol{B}}{\partial t} \\   
    \end{aligned}
\end{equation}

其中，$\nabla$为矢量微分算子(泊松算子)，$\boldsymbol{H}$ 为磁场强度，$\boldsymbol{J}$ 为电流密度，$\boldsymbol{D}$
为电位移，$\boldsymbol{E}$ 为电场强度，$\boldsymbol{B}$为磁通量密度， $t$为时间，$\rho$为区域内的电荷密度.
在各向同性的电磁场中，$\boldsymbol{J}$、$\boldsymbol{E}$、$\boldsymbol{B}$、$\boldsymbol{D}$、$\boldsymbol{H}$之间满足以下关系（介质的本构关系），
\begin{equation}
  \label{equation:Maxwell_relation}
  \begin{aligned}
    \boldsymbol{D} &= \varepsilon\boldsymbol{E}\\
    \boldsymbol{B} & = \mu\boldsymbol{E}\\
    \boldsymbol{J} & = \sigma\boldsymbol{E}\\
  \end{aligned}
\end{equation}
式中，$\varepsilon$是介电常数，$\mu$是介质的磁导率，$\sigma$是电导率。

为简化 EIT 问题的求解步骤，通常作以下假设：
\begin{enumerate}
  \item EIT 敏感场可被看作是似稳场。即 EIT 场域内部任意一点的电流在激励信号的作用下同
  时产生相应的变化。也就是说可以忽其场域内介电常数和位移电流的影响。
  \item EIT 场域内部不存在其他与外部激励信号频率相同的电流源。即场域内部不存在涡流效
  应，任意位置电流密度的散度为 0。
  \item 假设激励电流仅流经单排电极所在的平面。即研究二维 EIT 问题，以降低问题的复杂性，
  提高计算效率。
\end{enumerate}
根据以上假设，EIT 模型还满足：
\begin{equation}
  \label{equation:Maxwell_hp}
  \nabla \cdot \boldsymbol{J} = 0
\end{equation}
而电场强度$\boldsymbol{E}$和电位$\phi$之间满足
\begin{equation}
  \label{equation:Maxwell_EP}
  \boldsymbol{E} = - \nabla \phi
\end{equation}
则由式\cref{equation:Maxwell}、\cref{equation:Maxwell_EP}、\cref{equation:Maxwell_hp}、\cref{equation:Maxwell_relation}
可得拉普拉斯方程：
\begin{equation}
  \nabla \cdot (\sigma\nabla\phi) = 0
\end{equation}
上式即为EIT敏感场的数学模型。

在实际求解中，通常求解出其解析解（精确解）非常困难，因此通常使用基于有限元的迭代方法求解出其近似解，EIT有限元模型如\cref{figure:FEMa}所示
\begin{figure}[h]
\centering
\includegraphics[width=.5\textwidth]{FEM.png}
\caption{有限元模型}
\label{figure:FEMa}
\end{figure}

则可设单元场域为\cref{figure:fem}所示，三角形单元各顶点$M_1(x_1,y_1)$，$M_2(x_2,y_2)$，$M_3(x_3,y_3)$的电位分别为
$\Phi_1$,$\Phi_2$,$\Phi_3$。用向量表示为:
\[
  \Phi =
  \begin{pmatrix}
  \Phi_1 \\
  \Phi_2 \\
  \Phi_3
  \end{pmatrix}
  \]
\begin{figure}[htbp]
    \centering
    
  \begin{tikzpicture}[scale=1.5]
   
   
    % 坐标轴
    \draw[->] (-1,0) -- (3,0) node[right] {$x$};
    \draw[->] (0,-1) -- (0,3) node[above] {$y$};
    
    % 三角形
    \coordinate (M1) at (2,1);
    \coordinate (M2) at (2.5,2.5);
    \coordinate (M3) at (1.5,2);
  
    \draw  (M1) node[right] {$M_1(x_1,y_1)$} -- (M2) node[above right] {$M_2(x_2,y_2)$} -- (M3) node[above left] {$M_3(x_3,y_3)$} -- cycle;
  
  \end{tikzpicture}
  \caption{三角形单元}
  \label{figure:fem}
\end{figure}

令三角形单元内部的电位分布为：
\begin{equation}
  \Phi(x,y) = \alpha + \beta x + \gamma y 
\end{equation}
进一步将三角形顶点的坐标和电位带入求解可得：
\begin{equation}
  \Phi_e(x,y) = \frac{\Phi_1}{2\Delta}(a_1+b_1x+c_1y) + \frac{\Phi_2}{2\Delta}(a_2+b_2x+c_2y) + \frac{\Phi_3}{2\Delta}(a_3+b_3x+c_3y)
\end{equation}
其中，
\begin{equation}
  \left\{
  \begin{array}{c}
    a_1 = x_2y_3 - x_3y_2 \\
    a_2 = x_3y_1 - x_1y_3 \\
    a_3 = x_1y_2 - x_2y_1
  \end{array}
  \right.
  \qquad 
  \left\{
    \begin{array}{c}
      b_1 = y_2 - y_3 \\
      b_2 = y_3 - y_1 \\
      b_3 = y_1 - y_2
    \end{array}
    \right.
    \qquad 
    \left\{
      \begin{array}{c}
        c_1 = x_3 - x_2 \\
        c_2 = x_1 - x_3 \\
        c_3 = x_2 - x_1
      \end{array}
      \right.
\end{equation}
$\Delta = b_1c_2 - b_2c_1$为三角形的面积。
进一步令：
\begin{equation}
  B = \frac{1}{2\Delta} \left[
  \begin{array}{ccc}
    b_1 & b_2 & b_3 \\
    c_1 & c_2 & c_3 \\
    
  \end{array}
  \right]
\end{equation}
可求得三角形内部泛函为：
\begin{equation}
  F_e(\Phi) = \frac{1}{2} \Phi K_e \Phi^T - \Phi Q_e
\end{equation}
其中,
根据每个点的坐标可分别求得单元刚度矩阵$K$中各元素的值：
\begin{equation}
  K = 
  \left[
  \begin{array}{ccc}
    \frac{b_1^2 + c_1^2}{4\Delta \rho} & \frac{b_1b_2 + c_1c_2}{4\Delta \rho} & \frac{b_1b_3 + c_1c_3}{4\Delta \rho}  \\
    \frac{b_1b_2 + c_1c_2}{4\Delta \rho} & \frac{b_2^2 + c_2^2}{4\Delta \rho} & \frac{b_2b_3 + c_2c_3}{4\Delta \rho} \\
    \frac{b_1b_3 + c_1c_3}{4\Delta \rho} &  \frac{b_2b_3 + c_2c_3}{4\Delta \rho}& \frac{b_3^2 + c_3^2}{4\Delta \rho} 
  \end{array}
  \right]
\end{equation}
则待测长内总体泛函为
\boldmath
\begin{equation}
  F_e(\Phi) = \frac{1}{2} \Phi K \Phi^T - \Phi Q_e
\end{equation}
\unboldmath
进一步，对泛函取极值，得到线性方程组：
\boldmath
\begin{equation}
  K\Phi = Q
\end{equation}
\unboldmath
其中K为总体刚度矩阵，由单元刚度矩阵叠加而成\cite{2019d_cc}。由此可见，EIT正问题求解，主要通过变分法代替原问题，后将问题离散化一般的极值问题。
其对于边界形状任意变化的待测场也适用。也正是EIT正问题完备的仿真模型给利用深度学习模型实现EIT图像重建算法提供了数据基础。

\xsubsection{EIT逆问题}{Inverse Problem of EIT}
对于连续的已知平面$\Omega$上的EIT模型，其电压分布$u(x,y)$满足如下方程和边界条件：
\begin{equation}
  \begin{aligned}
  &\nabla \cdot (\sigma \nabla \phi) = 0, &(x, y)\in {\Omega} \\
  &\sigma\frac{\partial{u}}{\partial{v}} = \psi, &(x,y) \in {\partial{\Omega}} \\
  &\phi = \varphi, &(x,y)\in\partial{\Omega} \\
\end{aligned}
\end{equation}
其中，其内部电导率分布$\sigma(x,y) > 0$ ，边界电流密度$\psi(x,y)$，边界电压$\phi(x,y)$，边界电压$\varphi(x,y)$均是关于场域内部任意一点($x,y)$的函数。
若给定$\sigma(x,y)$和$\psi(x,y)$，则可由方程组确定出$u(x,y)$，即诺伊曼边界值问题。
若给定$\sigma(x,y)$和$\rho(x,y)$，求解$u(x,y)$，则为迪利克雷边界值问题。
上述两个问题称为EIT正问题。而实际EIT问题需要求解逆问题，即已知边界电压$\rho$和电流$\psi$求解电导率分布$\sigma(x,y)$。

电导率分布$\sigma(x,y)$确定了从可能的电流密度集合$\Psi$到和电压分布集合$\Phi$的映射。
\begin{equation}
  F_\sigma: \psi(x,y) \rightarrow \phi(x,y)
\end{equation}
EIT逆问题即是在已知 $F_{\sigma} = F_0$的条件下求解 $\sigma(x,y)$，使得
\begin{equation}
  F_{\sigma}(\psi) = F_{0}(\psi),  \forall \psi \in \Psi
\end{equation}
EIT逆问题在理论上存在唯一解已被先前的研究证实\cite{Sun1993An}。其条件则是已知$\rho(x,y),\quad x \in \partial \Omega$。
然而对于实际的工程问题而言，对于一个未知场，其可获得的边界电压值是有限的，即电极的个数限制了测量数据的维度。
这就导致EIT逆问题的严重病态性以及非适定性，进而影响了重建分辨率。
EIT逆问题的求解往往通过迭代的思想，根据每一步正问题求解后边界电压与真实测量结果的误差不断修正系统参数。
因此实现高质量EIT图像重建算法的关键即优化EIT逆问题求解的过程。

\xsection{深度学习相关理论}{Theory about Deep Learning}
\xsubsection{全连接神经网络}{Fully Connected Neural Networks}
全连接神经网络(fully connected network, FCN)，也被称为人工神经元网络（Artificial Neural Network，ANN）,多层感知机(multilayer Perceptron，MLP)。
该思想源于仿生学，其中树突对应MLP的输入层，轴突和轴突未梢对应其隐藏层和输出层。
FCN是深度学习模型中最基础的结构。一个三层的MLP包含了其模型如\cref{figure:FCN}所示，每层神经元本质上代表一组权重矩阵，用来对输入层的数据作线性变换，进而输出至下一层。
通常，输出层的数据会通过一个激活函数后输出最终结果。每层神经元的输出是下一层所有神经元的输入，故名“全连接”。
全连接网络有对非线性映射强大的拟合能力，故可以用来学习各种非线性映射。然而，随着神经网络层数的增加，其训练和推理过程由于网络参数规模庞大而往往更加耗时。
同时，全连接网络的输入通常为向量，因此会忽略掉二维或高维数据的维度特征。因此，当前主流的深度学习模型通常不以FCN作为主要模型架构，取而代之的是卷积神经网络等，以更好地适应复杂的任务和高维的数据。
FCN更多地是用作一个模型的一小部分。
\begin{figure}
  \usetikzlibrary{positioning}
  

  \begin{tikzpicture}[x=1.5cm, y=1.5cm, >=stealth]
  
  % 输入层
  \foreach \m/\l [count=\y] in {1,2,3}
      \node [circle,draw,fill=gray!20,minimum size=0.75cm] (input-\m) at (0,2.5-\y) {$x_\l$};
  
  % 隐藏层
  \foreach \m [count=\y] in {1,2,3}
      \node [circle,draw,fill=gray!50,minimum size=0.75cm] (hidden1-\m) at (2,2.5-\y) {$h_\m^{(1)}$};
  \foreach \m [count=\y] in {1,2,3}
      \node [circle,draw,fill=gray!50,minimum size=0.75cm] (hidden2-\m) at (4,2.5-\y) {$h_\m^{(2)}$};
  
  % 输出层
  \foreach \m [count=\y] in {1,2,3}
      \node [circle,draw,fill=gray!20,minimum size=0.75cm] (output-\m) at (6,2.5-\y) {$y_\m$};
  
  % 连接
  \foreach \m in {1,...,3}
      \foreach \n in {1,...,3}
          \draw [->] (input-\m) -- (hidden1-\n);
  \foreach \m in {1,...,3}
      \foreach \n in {1,...,3}
          \draw [->] (hidden1-\m) -- (hidden2-\n);
  \foreach \m in {1,...,3}
      \foreach \n in {1,...,3}
          \draw [->] (hidden2-\m) -- (output-\n);
  
  \end{tikzpicture}
  \caption{全连接神经网络}
  \label{figure:FCN}
\end{figure}







\xsubsection{卷积神经网络}{Convolutional Neural Networks}
