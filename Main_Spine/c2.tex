% !TeX root = ../main.tex

\xchapter{生成式EIT图像重建算法理论基础}{Theory of Generative EIT Reconstruction Algorithm}

EIT 技术是一种根据场域内部电导率分布情况获取场域内部信息的成像技术。该技术由
于其非侵入、无损伤等优势，近年来得到了广泛的应用。然而，EIT 图像重建算法由于其固
有的不足而限制的其使用场景。因此，如何提高重建图像的分辨率，并加快重建的速度，是
目前 EIT 技术的瓶颈。

近年来，深度学习技术，尤其是生成式模型，取得了突破性的成果，这也为提高 EIT 图
像重建质量提供了新的研究方向。不同与计算机视觉领域的其他图像生成任务（如利用文本
生成图像、图像分类、图像识别等），EIT 技术具有完备的数学物理模型，这就为基于深度学
习的 EIT 图像重建模型提供了大量的仿真数据支持。然而，由于其性能直接影响了临床医生
对于病情的判断，因此 EIT 图像重建算法的广泛应用需要确保其效率及可行性。

本章将主要介绍 EIT 图像重建算法的数学原理，深度学习和生成式模型的常见构件及模
型结构。随后将重点对两者之间结合的可行性以及优势作出阐述，为构建基于生成式模型的
EIT 图像重建算法提供理论基础。

\xsection{EIT基本原理}{Basic Principle of EIT}

\xsubsection{EIT问题模型}{Model of EIT Problem}
在工程数学领域通常对事物内部的特征进行参数化，用模型参数$\boldsymbol{m}$来表示，它属于模型参数域$M$。
这些参数在工程实践中通常难以观测得到。
因此会选取一些与该模型参数有显著相关性的可观测参数$\boldsymbol{d}$来推断得到模型参数，
其中$\boldsymbol{d}$属于可测量参数域$D$。
则正问题可被描述为通过已知的模型参数$\boldsymbol{m}$求解可测量参数$\boldsymbol{d}$，即\cref{equation:forward}
\begin{equation}
  \label{equation:forward}
  \boldsymbol{d} = h(\boldsymbol{m})
\end{equation}
特别的，线性正问题可被描述为\cref{equation:linear_forward}
\begin{equation}
  \label{equation:linear_forward}
  \boldsymbol{d} = \boldsymbol{H}\boldsymbol{m}
\end{equation}
其中，$\boldsymbol{H}$为矩阵算子。
则逆问题则被描述为已知测量参数$\boldsymbol{d}$求解模型参数$\boldsymbol{m}$的过程。

然而，在实际的工程中，往往无法直接获得逆问题的精确解，或根本不可能获得唯一的精确解，
即不可能满足\cref{equation:error}这是由于实际问题中的每一步都存在一定的误差。
\begin{equation}
  \label{equation:error}
  h(\boldsymbol{m}) - \boldsymbol{d} = 0
\end{equation}

特别的，对于EIT而言，其成像目标是待测场内部的电导率分布。该技术通过向未知场内注入电流，同时测
量其边界电压分布，利用麦克斯韦方程组求解出其场域内部的电导率（或阻抗，注：由于电
导率和电阻抗之间为倒数关系，可相互转换。虽 EIT 名为电阻抗断层成像，但在实际 EIT 的
研究中，通常利用电导率分布作为重建图像的直接依据，故如无特殊说明，本文的成像目标
均为电导率）分布，最后根据其内部的电导率分布重建出图像。
EIT 问题的研究可分为 EIT 正问题和 EIT 逆问题。EIT 正
问题是指已知待测场内部的电导率分布及其边界电流驱动信号，求解其内部或边界的电压或
电流分布。EIT 逆问题则是指已知场域边界的测量电压以及驱动信号，求解场域内部的电导
率分布情况。
\xsubsection{EIT正问题}{Forward Problem of EIT}
EIT 问题属于电磁场分析问题，其电导率分布、注入电流以及测量电压之间可根据麦克斯
韦(Maxwell)方程组
\begin{equation}
\label{equation:Maxwell}
    \begin{aligned}
        \nabla \times \boldsymbol{H} &= J + \frac{\partial{D}}{\partial{t}} \\
        \nabla \cdot \boldsymbol{D} &= \rho \\
        \nabla \cdot \boldsymbol{B} &= 0 \\
        \nabla \times\boldsymbol{E} &= -\frac{\partial \boldsymbol{B}}{\partial t} \\   
    \end{aligned}
\end{equation}

其中，$\nabla$为矢量微分算子(泊松算子)，$\boldsymbol{H}$ 为磁场强度，$\boldsymbol{J}$ 为电流密度，$\boldsymbol{D}$
为电位移，$\boldsymbol{E}$ 为电场强度，$\boldsymbol{B}$为磁通量密度， $t$为时间，$\rho$为区域内的电荷密度.
在各向同性的电磁场中，$\boldsymbol{J}$、$\boldsymbol{E}$、$\boldsymbol{B}$、$\boldsymbol{D}$、$\boldsymbol{H}$之间满足以下关系（介质的本构关系），
\begin{equation}
  \label{equation:Maxwell_relation}
  \begin{aligned}
    \boldsymbol{D} &= \varepsilon\boldsymbol{E}\\
    \boldsymbol{B} & = \mu\boldsymbol{E}\\
    \boldsymbol{J} & = \sigma\boldsymbol{E}\\
  \end{aligned}
\end{equation}
式中，$\varepsilon$是介电常数，$\mu$是介质的磁导率，$\sigma$是电导率。

为简化 EIT 问题的求解步骤，通常作以下假设：
\begin{enumerate}
  \item EIT 敏感场可被看作是似稳场。即 EIT 场域内部任意一点的电流在激励信号的作用下同
  时产生相应的变化。也就是说可以忽其场域内介电常数和位移电流的影响。
  \item EIT 场域内部不存在其他与外部激励信号频率相同的电流源。即场域内部不存在涡流效
  应，任意位置电流密度的散度为 0。
  \item 假设激励电流仅流经单排电极所在的平面。即研究二维 EIT 问题，以降低问题的复杂性，
  提高计算效率。
\end{enumerate}
根据以上假设，EIT 模型还满足：
\begin{equation}
  \label{equation:Maxwell_hp}
  \nabla \cdot \boldsymbol{J} = 0
\end{equation}
而电场强度$\boldsymbol{E}$和电位$\phi$之间满足
\begin{equation}
  \label{equation:Maxwell_EP}
  \boldsymbol{E} = - \nabla \phi
\end{equation}
则由式\cref{equation:Maxwell}、\cref{equation:Maxwell_EP}、\cref{equation:Maxwell_hp}、\cref{equation:Maxwell_relation}
可得拉普拉斯方程：
\begin{equation}
  \nabla \cdot (\sigma\nabla\phi) = 0
\end{equation}
上式即为EIT敏感场的数学模型。

在实际求解中，通常求解出其解析解（精确解）非常困难，因此通常使用基于有限元的迭代方法求解出其近似解，EIT有限元模型如\cref{figure:FEMa}所示
\begin{figure}[h]
\centering
\includegraphics[width=.5\textwidth]{FEM.png}
\caption{有限元模型}
\label{figure:FEMa}
\end{figure}

则可设单元场域为\cref{figure:fem}所示，三角形单元各顶点$M_1(x_1,y_1)$，$M_2(x_2,y_2)$，$M_3(x_3,y_3)$的电位分别为
$\Phi_1$,$\Phi_2$,$\Phi_3$。用向量表示为:
\[
  \Phi =
  \begin{pmatrix}
  \Phi_1 \\
  \Phi_2 \\
  \Phi_3
  \end{pmatrix}
  \]
\begin{figure}[htbp]
    \centering
    
  \begin{tikzpicture}[scale=1.5]
   
   
    % 坐标轴
    \draw[->] (-1,0) -- (3,0) node[right] {$x$};
    \draw[->] (0,-1) -- (0,3) node[above] {$y$};
    
    % 三角形
    \coordinate (M1) at (2,1);
    \coordinate (M2) at (2.5,2.5);
    \coordinate (M3) at (1.5,2);
  
    \draw  (M1) node[right] {$M_1(x_1,y_1)$} -- (M2) node[above right] {$M_2(x_2,y_2)$} -- (M3) node[above left] {$M_3(x_3,y_3)$} -- cycle;
  
  \end{tikzpicture}
  \caption{三角形单元}
  \label{figure:fem}
\end{figure}

令三角形单元内部的电位分布为：
\begin{equation}
  \Phi(x,y) = \alpha + \beta x + \gamma y 
\end{equation}
进一步将三角形顶点的坐标和电位带入求解可得：
\begin{equation}
  \Phi_e(x,y) = \frac{\Phi_1}{2\Delta}(a_1+b_1x+c_1y) + \frac{\Phi_2}{2\Delta}(a_2+b_2x+c_2y) + \frac{\Phi_3}{2\Delta}(a_3+b_3x+c_3y)
\end{equation}
其中，
\begin{equation}
  \left\{
  \begin{array}{c}
    a_1 = x_2y_3 - x_3y_2 \\
    a_2 = x_3y_1 - x_1y_3 \\
    a_3 = x_1y_2 - x_2y_1
  \end{array}
  \right.
  \qquad 
  \left\{
    \begin{array}{c}
      b_1 = y_2 - y_3 \\
      b_2 = y_3 - y_1 \\
      b_3 = y_1 - y_2
    \end{array}
    \right.
    \qquad 
    \left\{
      \begin{array}{c}
        c_1 = x_3 - x_2 \\
        c_2 = x_1 - x_3 \\
        c_3 = x_2 - x_1
      \end{array}
      \right.
\end{equation}
$\Delta = b_1c_2 - b_2c_1$为三角形的面积。
进一步令：
\begin{equation}
  B = \frac{1}{2\Delta} \left[
  \begin{array}{ccc}
    b_1 & b_2 & b_3 \\
    c_1 & c_2 & c_3 \\
    
  \end{array}
  \right]
\end{equation}
可求得三角形内部泛函为：
\begin{equation}
  F_e(\Phi) = \frac{1}{2} \Phi K_e \Phi^T - \Phi Q_e
\end{equation}
其中,
根据每个点的坐标可分别求得单元刚度矩阵$K$中各元素的值：
\begin{equation}
  K = 
  \left[
  \begin{array}{ccc}
    \frac{b_1^2 + c_1^2}{4\Delta \rho} & \frac{b_1b_2 + c_1c_2}{4\Delta \rho} & \frac{b_1b_3 + c_1c_3}{4\Delta \rho}  \\
    \frac{b_1b_2 + c_1c_2}{4\Delta \rho} & \frac{b_2^2 + c_2^2}{4\Delta \rho} & \frac{b_2b_3 + c_2c_3}{4\Delta \rho} \\
    \frac{b_1b_3 + c_1c_3}{4\Delta \rho} &  \frac{b_2b_3 + c_2c_3}{4\Delta \rho}& \frac{b_3^2 + c_3^2}{4\Delta \rho} 
  \end{array}
  \right]
\end{equation}
则待测长内总体泛函为
\boldmath
\begin{equation}
  F_e(\Phi) = \frac{1}{2} \Phi K \Phi^T - \Phi Q_e
\end{equation}
\unboldmath
进一步，对泛函取极值，得到线性方程组：
\boldmath
\begin{equation}
  K\Phi = Q
\end{equation}
\unboldmath
其中K为总体刚度矩阵，由单元刚度矩阵叠加而成\cite{2019d_cc}。由此可见，EIT正问题求解，主要通过变分法代替原问题，后将问题离散化一般的极值问题。
其对于边界形状任意变化的待测场也适用。也正是EIT正问题完备的仿真模型给利用深度学习模型实现EIT图像重建算法提供了数据基础。

\xsubsection{EIT逆问题}{Inverse Problem of EIT}
对于连续的已知平面$\Omega$上的EIT模型，其电压分布$u(x,y)$满足如下方程和边界条件：
\begin{equation}
  \begin{aligned}
  &\nabla \cdot (\sigma \nabla \phi) = 0, &(x, y)\in {\Omega} \\
  &\sigma\frac{\partial{u}}{\partial{v}} = \psi, &(x,y) \in {\partial{\Omega}} \\
  &\phi = \varphi, &(x,y)\in\partial{\Omega} \\
\end{aligned}
\end{equation}
其中，其内部电导率分布$\sigma(x,y) > 0$ ，边界电流密度$\psi(x,y)$，边界电压$\phi(x,y)$，边界电压$\varphi(x,y)$均是关于场域内部任意一点($x,y)$的函数。
若给定$\sigma(x,y)$和$\psi(x,y)$，则可由方程组确定出$u(x,y)$，即诺伊曼边界值问题。
若给定$\sigma(x,y)$和$\rho(x,y)$，求解$u(x,y)$，则为迪利克雷边界值问题。
上述两个问题称为EIT正问题。而实际EIT问题需要求解逆问题，即已知边界电压$\rho$和电流$\psi$求解电导率分布$\sigma(x,y)$。

电导率分布$\sigma(x,y)$确定了从可能的电流密度集合$\Psi$到和电压分布集合$\Phi$的映射。
\begin{equation}
  F_\sigma: \psi(x,y) \rightarrow \phi(x,y)
\end{equation}
EIT逆问题即是在已知 $F_{\sigma} = F_0$的条件下求解 $\sigma(x,y)$，使得
\begin{equation}
  F_{\sigma}(\psi) = F_{0}(\psi),  \forall \psi \in \Psi
\end{equation}
EIT逆问题在理论上存在唯一解已被先前的研究证实\cite{Sun1993An}。其条件则是已知$\rho(x,y),\quad x \in \partial \Omega$。
然而对于实际的工程问题而言，对于一个未知场，其可获得的边界电压值是有限的，即电极的个数限制了测量数据的维度。
这就导致EIT逆问题的严重病态性以及非适定性，进而影响了重建分辨率。
EIT逆问题的求解往往通过迭代的思想，根据每一步正问题求解后边界电压与真实测量结果的误差不断修正系统参数。
因此实现高质量EIT图像重建算法的关键即优化EIT逆问题求解的过程。

\xsection{深度学习相关理论}{Theory of Deep Learning}
近年来深度学习技术发展迅速，各种架构层出不穷，而本文所提出的EIT图像重建算法以深度学习模型为基础，因此本章将重点介绍该算法中所用到的深度学习技术的原理和架构。
\xsubsection{全连接神经网络}{Fully Connected Neural Networks}
全连接神经网络(fully connected network, FCN)，也被称为人工神经元网络（Artificial Neural Network，ANN）,多层感知机(multilayer Perceptron，MLP)。
该思想源于仿生学，其中树突对应MLP的输入层，轴突和轴突未梢对应其隐藏层和输出层。
FCN是深度学习模型中最基础的结构。一个三层的MLP包含了其模型如\cref{figure:FCN}所示，每层神经元本质上代表一组权重矩阵，用来对输入层的数据作线性变换，进而输出至下一层。
通常，输出层的数据会通过一个激活函数后输出最终结果。每层神经元的输出是下一层所有神经元的输入，故名“全连接”。
全连接网络有对非线性映射强大的拟合能力，故可以用来学习各种非线性映射。然而，随着神经网络层数的增加，其训练和推理过程由于网络参数规模庞大而往往更加耗时。
同时，全连接网络的输入通常为向量，因此会忽略掉二维或高维数据的维度特征。因此，当前主流的深度学习模型通常不以FCN作为主要模型架构，取而代之的是卷积神经网络等，以更好地适应复杂的任务和高维的数据。
FCN更多地是用作一个模型的一小部分。
\begin{figure}
  \usetikzlibrary{positioning}
  

  \begin{tikzpicture}[x=1.5cm, y=1.5cm, >=stealth]
  
  % 输入层
  \foreach \m/\l [count=\y] in {1,2,3}
      \node [circle,draw,fill=gray!20,minimum size=0.75cm] (input-\m) at (0,2.5-\y) {$x_\l$};
  
  % 隐藏层
  \foreach \m [count=\y] in {1,2,3}
      \node [circle,draw,fill=gray!50,minimum size=0.75cm] (hidden1-\m) at (2,2.5-\y) {$h_\m^{(1)}$};
  \foreach \m [count=\y] in {1,2,3}
      \node [circle,draw,fill=gray!50,minimum size=0.75cm] (hidden2-\m) at (4,2.5-\y) {$h_\m^{(2)}$};
  
  % 输出层
  \foreach \m [count=\y] in {1,2,3}
      \node [circle,draw,fill=gray!20,minimum size=0.75cm] (output-\m) at (6,2.5-\y) {$y_\m$};
  
  % 连接
  \foreach \m in {1,...,3}
      \foreach \n in {1,...,3}
          \draw [->] (input-\m) -- (hidden1-\n);
  \foreach \m in {1,...,3}
      \foreach \n in {1,...,3}
          \draw [->] (hidden1-\m) -- (hidden2-\n);
  \foreach \m in {1,...,3}
      \foreach \n in {1,...,3}
          \draw [->] (hidden2-\m) -- (output-\n);
  
  \end{tikzpicture}
  \caption{全连接神经网络}
  \label{figure:FCN}
\end{figure}

\xsubsection{卷积神经网络}{Convolutional Neural Networks}
卷积神经网络(Convolutional Neural Networks, CNN)是目前使用范围最广的深度学习模型之一，由其对高维度数据特征提取的能力优异表现而广泛应用于图像处理和计算机视觉任务中。
一般的CNN通常包含卷积层，池化层，全连接层。其常见的模型结构如所示。\cref{figure:CNN}

\begin{figure}[h]
  \centering
\includegraphics[width=0.6\textwidth]{CNN.JPG}
\caption{卷积神经网络结构示意}
\label{figure:CNN}
\end{figure}

卷积层通常利用卷积操作提取图像（或其他输入数据）的空间特征信息。CNN每一层通常具有多个通道（包含多个卷积核），每个卷积和分别与图像进行卷积运算，其的结果将会作为下一层的输入；不同的卷积核可以提取到图像不同类型的空间特征，因此CNN通常相较于FCN有更强大的特征提取能力。
池化层通常用于减小图像尺寸，同时可以保留输入结果中重要的特征信息。常见的池化操作通常有平均值池化(Average Pool)、最大值池化(Max Pool)等。
全连接层则是将CNN的输出结果进行重塑，使得输出维度符合实际需要。通常用于分类或回归预测等任务中。
CNN的运算原理如\cref{equation:CNN}所示。
\begin{equation}
  \label{equation:CNN}
  Y = \sigma\left(\sum_{i=1}^{n} \boldsymbol{W_i} conv(\boldsymbol{X_i}) + \boldsymbol{b}\right)
\end{equation}

其中：
$\mathbf{Y}$ 是输出特征图；
$\sigma(\cdot)$ 是激活函数；
$n$ 是卷积核的数量；
$\mathbf{W}_i$ 是第 $i$ 个卷积核的权重；
$\mathbf{X}_i$ 是与第 $i$ 个卷积核进行卷积的输入特征图；
$\mathbf{b}$ 是偏置项；
$conv(\cdot)$ 表示卷积操作。
自Resnet提出以来\cite{2016Resnet}，残差卷积以广泛应用在各种基于卷积神经网络的模型之中其核心思想是通过残差连接来解决神经网络中的梯度消失以及梯度爆炸问题。其模型如\cref{figure:Resnet}
通常这种结构可以使得网络直接学习残差（即残差块的输出减去输入）。本文大量采用了残差卷积块作为模型的基础模块，这种方式可以使整个网络更容易优化和训练，进一步提高网络的性能和泛化能力。
\begin{figure}
  \centering
  \includegraphics[width=0.6\textwidth]{Resnet.png}
  \caption{残差卷积模型}
  \label{figure:Resnet}
\end{figure}



\xsubsection{激活函数}{Activation Functions}

神经网络在各个领域的广泛应用主要得益于其对于非线性映射的强大拟合能力，
这种拟合能力，主要来自于非线性激活函数。
而深度学习模型如CNN，FCN等基础模块的运算通常为大量的线性映射（加法和乘法）。
因此，选择适当的激活函数，对于深度学习模型性能的提升将是显著的。
近年来，各式各样的激活函数不断被学者们提出，本文将重点介绍模型从采用的四种激活函数，如
对于其他未被使用到的激活函数将不做赘述。

\begin{enumerate}

  \item ReLU

ReLU是Hinton于2010年提出的激活函数\cite{2010ReLU}，其表达式如\cref{equation:ReLU}所示。
sigmoid函数在函数值趋近于0和1的时候会变得平坦，即此时的梯度几乎为0。而由于神经网络参数更新的方式通过反向传播进行，则此时对于这类输入或输出趋于0或的节点，其权重几乎不会得到更新，此类神经元被称为饱和神经元。
因此，这就会导致神经网络中许多节点的参数不再更新，即梯度消失问题。
ReLU在输入大于0的时候其梯度恒为1，因此可以有效避免梯度消失问题，并且由于其中只存在线性映射，因此能大大提高网络的计算效率，进而加快梯度下降过程中的收敛速度。
此外，该激活函数还被认为具有生物和理性，即与单侧抑制原理相符。

  \begin{equation}
    \label{equation:ReLU}
    \sigma(x) = 
    \left\{
    \begin{aligned}
      0, \quad x <= 0 \\
      x, \quad x > 0
    \end{aligned}
    \right.
    \end{equation}

  \item Sigmoid Linear Unit (SiLU)
  
    SiLU的表达式如\cref{equation:SiLU}所示。
    其相较于ReLU而言，由于其连续可微，故在反向传播的过程中更容易优化。并且SiLU在其定义域内梯度均不为0，这非常有助于在深度神经网络中避免梯度消失的问题。
    同时，由于ReLU函数的特性，网络中输出结果小于0的神经元会被抑制（其梯度为0），故会造成网络稀疏，虽然有助于抑制过拟合，但是也会抑制网络对于数据信息的利用率，进而影响模型的性能。
    并且由于ReLU和Sigmoid函数的输出数据分布的均值均不为零，因此会给给后一层的神经网络引入偏置偏移，进而可能会影响梯度下降的效率。
    而，SiLU的输出范围在$[0, 1]$之间，因此可以为后续的模块保留一定的动态范围，从而有助于更好地处理数据分布的变化。

    \begin{equation}
      \label{equation:SiLU}
      \text{SiLU}(x) = x \cdot \sigma(x)
    \end{equation}
  
    其中，$\sigma(x)$ 是 Sigmoid 函数，定义为：
  \begin{equation}
    \sigma(x) = \frac{1}{1 + e^{-x}}
  \end{equation}
  
  \item Gaussian Error Linear Units function (GeLU)
  
  GeLU是2016年 Hendrycks等人提出的激活函数\cite{2016GeLU}。但直到其在Transformer和Bert模型中得到应用之后，才广泛进入人们的视野。
  该函数的表达式如\cref{equation:GeLU}。
  其中，$\Phi(x)$ 是标准正态分布的累积分布函数（CDF），可以用高斯误差函数（erf）的形式表示。
  GeLU函数通过给输入向量乘以一个权重，该权重是输入变量的非线性映射（即高斯误差函数），其原理与ReLU和Dropout类似。
  不同的是，ReLU会为输入向量乘以固定的权重1（或0），而dropout则会为部分输入乘以0（即抛弃该输入）。
  GeLU所乘的系数，这是与输入变量的自身分布有关。由此，GeLU可以将输入变量$x$平滑地映射到高斯分布的CDF上。
  重要的是，GeLU在最近的深度学习模型中通常表现出优于ReLU或ELU等传统的激活函数。

  \begin{equation}
    \label{equation:GeLU}
    \text{GELU}(x) = x \cdot \Phi(x) 
  \end{equation}
   \begin{equation}
    \label{equation:Gauss}
    \Phi(x) = \frac{1}{2} \left(1 + \text{erf} \left(\frac{x}{\sqrt{2}}\right)\right)
  \end{equation}
  
  特别注意的是，在实现的过程中可以用如\cref{equation:quickGeLU}所示的函数对GeLU作近似估计：
  \begin{equation}
    \label{equation:quickGeLU}
    \text{quickGELU}(x) = \frac{1}{2}x\left(1 + \tanh\left(\sqrt{\frac{2}{\pi}}\left(x + 0.044715x^3\right)\right)\right)
  \end{equation}
  其中，
  \begin{equation}
    tanh(x) = \frac{{e^{x} - e^{-x}}}{{e^{x} + e^{-x}}}
\
  \end{equation}

  \item Softmax

  Softmax函数不同于其他激活函数，其作用是将连续的输出信号映射到离散的空间中，故经常用作分类网络的输出层，其表计算方式如\cref{Softmax}所示。
  Softmax计算该层每个神经元的输出对所有神经元输出的和的比值。即此过程将每个神经元的输出映射到了$[0,1]$中，并且输出层
  所有的神经元之和为1。这样做的好处则是输出层的神经元可以表示为一个概率空间，其中每个神经元的输出可以看做是某一个事件发生的概率。
  因此经常作为多分类问题的输出层使用。

\begin{equation}
  \label{softmax}
  \text{softmax}(\mathbf{z})_i = \frac{e^{z_i}}{\sum_{j=1}^{K} e^{z_j}}
\end{equation}

其中，$\mathbf{z} = (z_1, z_2, ..., z_K)$ 是输入向量，$K$ 是类别的数量。softmax 函数将输入向量的每个元素 $z_i$ 转换为相应的概率值，即满足所有概率之和等于1。

\end{enumerate}
\begin{figure}[h]
  \centering
\includegraphics[width=0.9\textwidth]{activation.png}
\caption{激活函数}
\label{figure:activation}
  
\end{figure}

\xsubsection{归一化}{Normalization}
在机器学习（尤其是深度学习）任务中，由于不同通道（特征）数据的量纲往往不同，而这种情况通常会影响最终数据分析的结果。
因此，为了消除这种由于量纲不同而导致模型性能下降的可能性，通常会进行数据标准化（standardization），最常用的方法即数据归一化（Normalization）处理。
简而言之，归一化的目的就是使得预处理的数据被限定在一定的范围内（比如$[0,1]$或者$[-1,1]$），从而消除奇异样本数据导致的不良影响。

本文将重点介绍模型中使用到的以下三种归一化：
\begin{enumerate}
  \item 层归一化（Layer Normalization）
  \item 批归一化（Batch Normalization）
  \item 组归一化（Group Normalization）
\end{enumerate}

\xsubsection{优化器}{optimizer}

\xsubsection{注意力机制}{attention}
注意力机制(Attention)是由 Bahdanau等人于2014年提出的\cite{2014Neural}，最初应用于自然语言处理领域，此后由于其性能优秀而广泛应用在各个领域。
attention机制让神经网络能够关注输入数据的某些相关的信息，而减少对无用信息的关注。这种思想源于人们的视觉系统，即人们在看到一张图片或视频时通常会关注其中感兴趣的部分。

attention通常包含三个重要的部分，即：
\begin{enumerate}
  \item 查询矩阵（query, Q）： 其中的值代表模型应该更加关注输入数据中的某些部分，即Q作为输入向量的某种表示，可以反映模型更加需要关注的内容。
  \item 键（key, K）：表示输入向量的特征。
  \item 值（values, V）：是K对应输入向量的另一维度的特征，用于和QK所获得的的相似度进行加权和。
\end{enumerate}

其基本原理为：首先计算出Q和K的相似度，然后利用Softmax进行归一化算出每个元素所对应各权重，最后利用上一步计算的结果与V中的元素做加权和，以输出最终的结果。
其中，由于相似度计算方式的不同以及Q、K、V选择方式的不同，就产生了attention的各种应用以及变体，包括自注意力机制（Self-attention），交叉注意力机制（Cross-attention）等。


\xsection{生成式模型的原理及架构}{Principle and Architecture of the Generative Model}
\

\

