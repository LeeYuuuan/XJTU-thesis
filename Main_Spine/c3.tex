% !TeX root = ../main.tex

\xchapter{生成式EIT图像重建算法研究}{Research on Generative EIT Reconstruction Algorithm}
EIT图像重建算法由于其固有的病态性而导致其图像重建质量差、重建分辨率低。而生成式模型目前以广泛应用在图像重建、图像生成等领域。本章深入研究了EIT图像重建问题的难点以及将生成式模型应用在EIT图像重建算法上的可行性。
并提出了一个生成式EIT图像重建算法。
\xsection{EIT图像重建问题分析}{Analysis of EIT Image Reconstruction Problem}
传统的EIT图像重建算法通常利用迭代的思想多次求解EIT正问题、逆问题，以不断最小化每一步求解的电压分布和真实电压分布之间的误差。
这种做法通常消耗时间长，成像分辨率低。由于EIT正问题仿真模型完备，且相对精确，而常见的深度学习模型则需要大量的训练数据，
因此基于深度学习的EIT图像重建模型大大提高了EIT图像重建算法的图像重建分辨率。
并且由于训练好的深度学习模型正向推理的过程通常耗时较少，因而能显著提高图像重建算法的时间效率。
这也为EIT技术实现床旁实时监测、院前疾病检测提供了技术保障。

EIT图像重建算法就是要利用场域边界电压-电流数据重构出其内部电导率分布。
但是，电流的激励方式、电压信号的维度等都会影响到EIT图像重建算法的设计，并且同一类型的数据也包含多种表示方式（如电压可表示为矩阵形式和向量形式）。
因此本节首先定性和定量地描述EIT图像重建问题，随后将提出一个生成式EIT图像重建算法。

设如\cref{figure:xyz}所示的待测场$\Omega$所包含的剖分单元个数为$d_e$，边界为$\partial \Omega$。其边界的电极个数为$d_v$。
EIT采集系统的激励模式为邻近激励，单次测量所有激励模式所构成的集合为$S_{stim}$，单次测量电流激励的次数为$n_m$；测量电压的模式为邻近差分，单次激励测量电压的维度为$d_{vm}$。
则单次测量的电压向量维度为$d_m = d_{vm} * n_m$。（关于EIT的有限元模型和测量模式详见\cref{PrincipleEIT}）。
利用上述测量模式，可获得EIT测量电压分布。EIT的测量电压共包括两个部分，分别为背景帧的边界电压以及含扰动目标的边界电压。
其中，背景帧的边界电压分布可分别表为矩阵形式$v_{0mtx} = \{v_{ij}\}, i \in S_{stim}, j =1,2,...d_{vm}$，向量形式$v_{0vector}$。
含扰动目标的边界电压分步的两种形式分别为$v_{mtx} = \{v_{ij}\}, i \in S_{stim}, j =1,2,...d_{vm}$。向量形式$v_{vector}$，
其中$v_{ij}$表示以$i$为激励方式的第$j$次测量的结果，向量的维度为$d_m$。

\begin{figure}[h]
    \centering
    \includegraphics[width=.5\textwidth]{eeeee.png}
    \caption{待测场图示}
    \label{figure:xyz}
  \end{figure}


则EIT图像重建算法的目标为通过测量含扰动目标的边界电压以及背景帧的边界电压的差$\Delta v_{vectore} = v_{vector} - v_{0vector}$重建出其场域内部的电导率分布变化向量 $\sigma$，其维度为 $1 \times d_e$。
EIT重建的图像分辨率为$res_{row} \times res_{column}$。

如无特殊说明，本文所采用的参数均为\cref{table:param_stm}中的默认值。
\begin{table}[h]
    \centering
    
    \caption{参数设置}
    \begin{tblr}{
        colspec = {X[c] X[c] X[c] X[c]},
    }
    \toprule
    参数 & 值(域) & 参数 & 值(域) \\
    \midrule
    $\Omega$ & 圆形场域 $ x \in [-1, 1], y \in[-1, 1]$ & $S_{stim}$ & [i, i+1], i=(0:15)\%15 \\
    $d_e$ & 1024 & $n_m$ & 16 \\
    $d_v$ & 16 & $d_{vm} $ & 16 \\
    $d_m$ & 256 & $\sigma_i$ & $[1, d_e]$ \\
    $res_{row}$ & 16 & $res_{column}$ & 16 \\

    \bottomrule
    \end{tblr}
    \label{table:param_stm}
\end{table}

目前主流的利用深度学习技术实现的EIT图像重建算法通常以卷积神经网络的各种变体为模型基础，
通过添加具有不同功能的模块来优化网络性能。
此类方法通常使用判别式模型来实现。

扩散概率模型，作为生成式模型的一种，近年来在图像生成领域已经取得了显著的成果。而EIT图像重建问题本质上是建立从电压分布到电导率分布的非线性映射，
即可以看作是生成目标为电导率分布，且指导模型生成电导率分布的条件为电压分布的带条件的扩散概率模型。
利用扩散模型实现的EIT图像重建算法由于其模型固有的优势而能更好地拟合电导率的原始概率分布。并且通过对扩散过程的改进，可
以有效减少在推理过程中的采样次数，极大程度地缩短了模型正向计算的过程，进而提高了EIT图像重建效率。

本章将提出一个生成式EIT图像重建算法，并重点介绍其设计理念与实现过程。包括算法的流程、EIT图像重建模型总体的结构、模型各部分的结构和作用、
模型训练集的设计和仿真、训练过程、训练结果以及对模型性能的评估和优化。


\xsection{基于深度生成式模型的EIT图像重建算法}{EIT Reconstruction Algorithm Based-on Deep Generative Model}

根据上一节所提出的EIT图像重建问题，本节提出了一个用于脑卒中快速鉴别的生成式EIT图像重建算法，
由于其应用EC-Diffusion模型实现而命名为EC-Diffusion算法如\cref{algorithm:GeneAlgorithm}和\cref{figure:AlgorithmEIT}。


\begin{figure}[h]
    \centering
    \includegraphics[width=.8\textwidth]{AlgorithmEIT.png}
    \caption{生成式EIT图像重建算法}
    \label{figure:AlgorithmEIT}
\end{figure}

\begin{algorithm}[H]
    
    \caption{生成式EIT图像重建算法}
    \begin{algorithmic}[1]
        \State 测量未知场域不含扰动目标的边界电压$v_{0vector}$
        \State 测量含有扰动目标时该场域的边界电压$v_{vector}$
        \State 计算边界电压差向量$\Delta v_{vector} = v_{vector - v_{0vector}}$
        \State 获得训练好的EC-Diffusion 模型 $\text{EC-Diffusion}(\cdot)$
        \State \textbf{Input:} $\Delta v_{vecotr}$
        \State \textbf{Output:} $ \sigma  = \text{EC-Diffusion}(\Delta v_{vecotr})$
    \end{algorithmic}
    \label{algorithm:GeneAlgorithm}
\end{algorithm}

在该算法中，“获得训练好的EC-Diffusion模型”共需要三个阶段实现，分别是：

1）设计并实现用于EIT图像重建任务的模型。该阶段使用改进的扩散模型作为EIT图像重建算法的核心模块，因此还需要对原始的
电压数据进行处理，故该阶段采用了一个编码器-解码器结构分别对重建前的电压数据进行编码（预处理）以及对重建后的电导率分布进行解码（后处理）。
由于该模型利用Diffusion Model通过编码后的电压分布重建出编码后的电导率分布，故将其命名为EC-Diffusion（Encoded Diffusion）。

2）模型训练。这一阶段会使用仿真软件仿真出用于训练模型所需要的EIT数据，并利用这些数据训练模型至收敛。

3）模型评估。将用于测试网络性能的数据输入至训练好的模型，并根据其重建出的图像以及其他参数评估模型性能并优化模型。

上述阶段利用EIT数据使EC-Diffusion模型学习到最优的参数，以尽可能地拟合EIT电压-电导率分布。
当以上阶段完成后，便可利用\cref{algorithm:GeneAlgorithm}解决EIT图像重建问题。即，
将EIT边界差分电压数据输入至训练好的模型中，经过模型正向推理即可得到其内部电导率分布的图像。




EC-Diffusion模型是实现本文所提出的EIT图像重建算法的核心部分，故本节将首先介绍该模型的总体架构，
随后将分别对该模型中电压编码块、图像重建块和图像后处理块的设计与实现展开详细的介绍。

\xsubsection{模型的总体架构}{Overall of the Model Architecture}
\label{section:Model}

针对EIT图像重建问题中的非线性映射，
本文提出了一个利用编码器-解码器以及一个基于改进扩散模型的深度生成式EIT图像重建模型：EC-Diffusion，如\cref{figure:EITREmodel}所示，

\begin{figure}[h]
    \centering
    \includegraphics[width=.9\textwidth]{EITREmodel.png}
    \caption{EC-Diffusion 模型}
    \label{figure:EITREmodel}
\end{figure}
该模型共包括三个模块，分别是：

1）电压编码块（Voltage Encoding Block）：由一个电压编码器（Voltage Encoder, VEncoder）构成。用于对电压数据进行编码，以输出适应图像重建块的输入维度。

2）图像重建块（即基于扩散模型的图像重建快，Diffusion-Based Reconsturction）。该部分利用改进的扩散概率模型拟合EIT图像重建问题中的非线性映射，输出编码后的电导率分布。该模块是本算法的核心部分。

3）图像后处理块（即电导率解码块，Conductivity Decoding Block）。由一个电导率解码器（Conductivity Decoder, CDecoder）构成。用于解码图像重建块的生成结果，将生成结果映射到用于引导EIT图像的电导率分布。

则该模型将首先利用VEncoder对EIT电压数据进行编码，随后将编码后的电压输入到图像重建块中，获得图像重建结果。需要注意的是，
该图像重建块重建出的结果同样为编码后的图像，因此还需要利用一个CDecoder对编码后的图像进行解码，解码后的结果即为该模型重建后的EIT图像。
以下将分别介绍上述三个模块的设计与实现。



\xsubsection{电压编码块}{Voltage Encoding Block}


VEncoder的编码目标是使得编码后的结果尽可能表达出更强有关于该电压数据所对应的电导率分布的特征，
而CNN对于高维数据的空间特征提取能力具有显著的优势，同时，CNN也相较于MLP而言具有更少的可学习参数。因此，此节以CNN为主体，利用径向基函数神经网络（Radial Basis Function Neural Network, RBFNN）
实现了一个RBF-CNN结构的编码器，其结构如\cref{figure:RBFCNN}所示。
\begin{figure}[h]
    \centering
    \includegraphics[width=.9\textwidth]{RBFCNN.png}
    \caption{VEncoder编码器架构}
    \label{figure:RBFCNN}
\end{figure}

其中，Conv3$\times$3为卷积层，卷积核大小为3$\times$3；Max pool 为最大池化层；Average pool为平均值池化层。

RBF则为RBFNN层，
与 MLP 不同，RBFNN 使用欧几里德距离和高斯激活函数，这使得神经元对局部信息更加敏感。 
RBF 网络具有与 MLP 类似的输入层和输出层。
不同的是，隐藏层中的每个神经元都有一个原型向量和一个由 $\mu$ 和 $sigma$ 表示的带宽分别计算输入向量与其原型向量之间的相似度，如\cref{equation:RBFNN}所示。
\begin{equation}
    \label{equation:RBFNN}
    y(x) = f(z(x)) = \sum_{i=1}^{m} v_i \phi_i(z(x)) + b
\end{equation}
其中：
\begin{equation}
    \phi_i(z(x)) = \exp\left(-\frac{|x - \mu_i|^2}{2\sigma_i^2}\right)
\end{equation}

$y(x)$ 是 RBFNN 的输出；
$f(\cdot)$ 是输出层的激活函数；$z(x)$ 是隐含层的输出；
$\phi_i(z(x))$ 是隐含层神经元 $i$ 的激活函数；
$v_i$ 是连接隐含层和输出层的权重；
$b$ 是偏置项；
$m$ 是隐含层神经元的数量。

SelfAttention层为自注意力机制层，由一个LayerNorm层 和一个SelfAttention模块和组合而成。
如\cref{figure:Selfatt}所示。
\begin{figure}[h]
    \centering
    \includegraphics[width=.9\textwidth]{Selfatt.png}
    \caption{SelfAttention 层}
    \label{figure:Selfatt}
\end{figure}

注意力机制(Attention)是由 Bahdanau等人于2014年提出的\cite{2014Neural}，最初应用于自然语言处理领域，此后由于其性能优异而广泛应用在各个领域。
注意力机制让神经网络能够关注输入数据的某些相关的信息，而减少对无用信息的关注。这种思想源于人们的视觉系统，即人们在看到一张图片或视频时通常会关注其中感兴趣的部分。

Attention通常包含三个重要的部分，即：

  1） 查询矩阵（query, Q）： 其中的值代表模型应该更加关注输入数据中的某些部分，即Q作为输入向量的某种表示，可以反映模型更加需要关注的内容。
  
  2） 键（key, K）：表示输入向量的特征。

  3） 值（values, V）：是K对应输入向量的另一维度的特征，用于和QK所获得的相似度进行加权和。


其基本原理为：首先计算出Q和K的相似度，然后利用Softmax进行归一化算出每个元素所对应的权重，最后利用上一步计算的结果与V中的元素做加权和，以输出最终的结果。
其中，由于相似度计算方式的不同以及Q、K、V选择方式的不同，就产生了Attention的各种应用以及变体，包括自注意力机制（Self-Attention），交叉注意力机制（Cross-Attention）等。

SelfAttention层是注意力机制的一种应用，其具体的计算过程如\cref{algorithm:Self_attention}。该机制允许模型在处理电导率分布时专注于数据的某些部分（如扰动目标的部分），而忽略其他部分（即背景帧）。
该模块可以使得电导率分布在不同的位置之间保持联系，以实现对扰动部分更好地成像。

\begin{algorithm}

\caption{Self Attention Layer}
\begin{algorithmic}[1]
    \State \textbf{Input:} 输入向量 $\boldsymbol{X}$代表网络中的隐变量。
    \State 初始化权重矩阵：$W_Q$, $W_K$, $W_V$
    \State 计算Q、K、V：$Q = XW_Q, \quad K = XW_K, \quad V = XW_V$
    \State 计算Attention：$A = \text{softmax}\left(\frac{QK^T}{\sqrt{d_k}}\right) \quad $ $d_k$ 为Q和K的维度 
    \State 计算加权后的结果：$Z = AV$
    \State 对Z进行线性变换，获得Self-Attention最终的输出 ：$\text{Self-Attention}(X) = ZW_O$
    \State \textbf{Output:} $\text{Self-Attention}(X)$
\end{algorithmic}
\label{algorithm:Self_attention}
\end{algorithm}

具体而言， 由于EIT图像重建任务中，其电导率分布按照扰动目标的存在与否通常可被分为两个部分：背景帧溶液和扰动目标部分。
其中，背景帧部分通常模拟电导率分布相对固定的电解质溶液，用于代表正常脑组织的区域。这部分通常对于EIT成像结果影响较小（即应该降低该部分对于EIT图像重建结果的影响），因此需要降低模型对于该部分的关注度。
而扰动目标区域则用于模拟特定的病灶区域（由于这些区域的含水量与背景帧不同而导致的电导率变化）。这部分通常对于EIT成像结果影响显著（即扩大该部分数据对于EIT重建结果的影响）。
而Self Attention机制通过计算一组数据内部的相关性并对每个元素重新赋上加权和，可以有效的关注其输入数据的特定部分。因此在此处引入Self Attention层将一定程度上提高扰动目标区域对电压分布或电导率分布中相关部分的关注程度，
进而可以有效地提高网络对EIT非线性映射的拟合能力。

LN层为层归一化层（Layer Normalization, LN），用于对上一层的输出归一化。
LN对该次输入所有通道的所有元素进行归一化（计算所有输入的均值和方差），然后利用\cref{equation:norm}对其进行更新。
\begin{equation}
  \label{equation:norm}
  res = \frac{x - E\left[ x\right]}{\sqrt{Var\left[ x\right] + \epsilon}} * \gamma + \beta
\end{equation}

其中，$E\left[ x\right], Var\left[ x\right]$分别是上述数据的均值和方差， $\epsilon$为一趋于0的常数，确保分母大于零，
$\gamma, \beta$是可学习的参数，用于对数据进行仿射变换。
由于该层在运算的过程中不受到batch size的限制，故可以有效应用在小批次的样本中。此外，该方法可看作是组归一化中组数为1的特殊情况。


此外，该编码器中的GELU层为激活函数层，利用GELU作为激活函数。在对GELU实现时，本文采用如\cref{equation:quickGeLU}所示的函数对GeLU作近似估计：
  \begin{equation}
    \label{equation:quickGeLU}
    \text{quickGELU}(x) = \frac{1}{2}x\left(1 + \tanh\left(\sqrt{\frac{2}{\pi}}\left(x + 0.044715x^3\right)\right)\right)
  \end{equation}
  其中，
  \begin{equation}
    tanh(x) = \frac{{e^{x} - e^{-x}}}{{e^{x} + e^{-x}}}
\
  \end{equation}
由于GELU本身运算复杂，因此该做法可有效提高网络的运算效率。


\xsubsection{图像重建块}{Image Reconstruction Block}

图像重建块的目标即为利用编码后的电压数据重建出编码后的电导率分布图像，如\cref{figure:ReconsBlock}所示。
\begin{figure}[h]
    \centering
    \includegraphics[width=.7\textwidth]{ReconsBlock.jpg}
    \caption{图像重建块}
    \label{figure:ReconsBlock}
\end{figure}

由于扩散模型在图像重建、图像生成、图像超分辨率等领域均取得了广泛的应用和显著的成绩，故此模块考虑利用扩散模型作为主体来拟合EIT图像重建过程。
原始的扩散模型通过对一组数据分布进行拟合，从而输出同分布的数据。而EIT图像重建任务中，电导率分布通常由电压分布引导而来，故该模型还需要通过电压数据引导，
从而生成特定电压分布下对应的电导率分布结果。以下将具体介绍该模型的实现方式。

对于EIT图像重建过程，本文建立了如下的扩散模型：

假设假设原始的电导率分布为$\mbs{x}_0$，原始电导率分布模型可以表述为
$p_\theta(\mbs{x}_0) \coloneqq \int p_\theta(\mbs{x}_{0:T})\diff x_{1:T}$。则扩散过程（Diffusion Process）和逆扩散过程（Reverse Process）可描述为如下方式：

\subsubsection{扩散过程}

扩散过程，也被称为正向过程（Forward Process），如\cref{figure:dp}。通过向原始的电导率分布中不断加入已知的高斯噪声，
随着噪声的不断增加该数据分布将趋近于一个高斯分布。

\begin{figure}[h]
    \centering
    \includegraphics[width=.95\textwidth]{DP.jpg}
    \caption{正向过程}
    \label{figure:dp}
\end{figure}

该过程通过一个用一个控制噪声与信号混合比例的方差序列（Variance Schedule）$\beta_1, ...\beta_T$，向电导率分布中逐渐添加高斯噪声。满足$\beta_1 < \beta_2 <...<\beta_T$。

由于马尔科夫链中第$t$步的$\mbs{x}_{t}$只与$t-1$时刻的$\mbs{x}_{t-1}$有关，故可得给定原始电导率分布$\mbs{x}_0$时近似后验$q(\mbs{x}_{1:T}|\mbs{x}_0)$为：
\begin{equation}
    \centering
    q(\mbs{x}_{1:T}|\mbs{x}_0) = \prod_{t=1}^{T} q(\mbs{x}_{t}|\mbs{x_{t-1}}) \qquad
    q(\mbs{x}_{t}|\mbs{x}_{t-1}) = \mathcal{N}(\mbs{x}_{t}; \sqrt{1-\beta_t}\mbs{x}_{t-1}, \beta_t\mbs{I})
    \label{equation:ddpm_f}
  \end{equation}

其中，$t$为时间轴（线性），定义为$[0,1,2,...,1000]$。
$\mbs{x}_1, \mbs{x}_2,..,\mbs{x}_{t}$，$t=1:T$是第$t$步加噪声后的结果。如此可保证$\mbs{x}_{t}$在$t$时刻有较小的信噪比，
即近似后验$q(\mbs{x}_{T}|\mbs{x}_0)$接近于一个同维度的标准正态分布。

经推导可得出$t$时刻的隐变量$\mbs{x}_{t}$可由原始数据分布$\mbs{x}_0$得到：
\begin{equation}
  \centering
  q(x_t|x_0) = \mathcal{N}(x_t; \sqrt{\bar{\alpha_t}x_0}, (1-\bar{\alpha}\mbs{I}))
\end{equation}


\subsubsection{逆扩散过程}

逆扩散过程（Reverse Process）是一个可学习的马尔科夫链（Markov Chain）。如\cref{figure:revs}所示。

\begin{figure}[h]
    \centering
    \includegraphics[width=.95\textwidth]{revs.jpg}
    \caption{逆扩散过程}
    \label{figure:revs}
\end{figure}

该过程将从$T$时刻的隐变量$\mbs{x}_{t}$，
即一个高斯分布$p_\theta(\mbs{x_{0:T}}) = \mathcal{N}(\mbs{x}_{t}; \mbs{0},\mbs{I})$开始，通过不断去噪声，从而恢复出原始的电导率分布。
该过程定义为$p_\theta(x_{0:T})$，如\cref{equation:ddpm_r}，则只要得到$q(x_{t-1}|x_t)$，就可以从一个高斯噪声中还原出原本的数据点$x_0$的分布。
\begin{equation}
    \centering
    p_\theta(\mbs{x}_{T}) = \prod_{t=1}{T} q(\mbs{x}_{t-1}|\mbs{x}_{t}) \qquad
    p_\theta(\mbs{x}_{t-1}|\mbs{x}_{t}) = \mathcal{N}(\mbs{x}_{t-1}; \mu_\theta(\mbs{x}_{t}, t, \mbs{v}), \Sigma_\theta(\mbs{x}_{t}, t, \mbs{v}))
    \label{equation:ddpm_r}
\end{equation}

其中，$\mu_\theta(\mbs{x}_{t}, t, \mbs{v})$和$\Sigma_\theta(\mbs{x}_{t}, t, \mbs{v})$分别为$p_\theta(\mbs{x}_{t-1}|\mbs{x}_{t})$
的均值和方差。

由此可以看出，只要获得$\mu_\theta(\mbs{x}_{t}, t, \mbs{v})$和$\Sigma_\theta(\mbs{x}_{t}, t, \mbs{v})$，就可以根据上述马尔科夫链求解出
原始的电导率分布。

在上式中，经推导可得，均值
\begin{equation}
  \mu_\theta = \frac{1}{\sqrt{\alpha_t}}(\mbs{x}_{t} - \frac{\beta_t}{\sqrt{1-\hat{\alpha_t}}}\epsilon_\theta(\mbs{x}_{t}, t, v))  
\end{equation}
而方差则可由方差序列得到。
则进一步可以看出，只需要得到从$t$时刻到$t-1$时刻需要去掉的噪声$\epsilon_\theta(\mbs{x}_{t}, t, v)$，
即可递推地重建出$t=0$时刻的原始电导率分布$\mbs{x}_0$。故只需要利用神经网络拟合出所需要添加噪声的网络即可推理出原始的数据分布。

\subsubsection{图像重建块}

根据上述设计，本文首先设计并实现了一个用生成噪声的网络NoiseG，以对隐变量去噪。
随后按照如\cref{figure:ReconsModel}实现了上述模型。


\begin{figure}[h]
    \centering
    \includegraphics[width=1\textwidth]{ReconsModel.PNG}
    \caption{图像重建块模型结构}
    \label{figure:ReconsModel}
\end{figure}

在推理阶段时，首先随机生成一个大小为$32\times 32$的高斯噪声（与电导率分布编码后的大小相同）作为$t$时刻的隐变量$\mbs{x}_{t}$，
然后通过向训练好的NoiseG中输入该时刻的隐变量$\mbs{x}_t$、当前时刻$t$以及经过上一节中电压编码器VEncoder编码后的电压向量$\mbs{v}$，
输出上一时刻的隐变量$\mbs{x}_{t-1}$，经过不断迭代最终输出该电压分布作为边界条件所对应的电导率分布（编码后）。

NoiseG网络以U-net作为基本结构，如\cref{figure:Unet}所示。U-net是由Olaf Ronneberger等人于2015年提出的基于CNN的深度学习模型\cite{2015U}，该网络在医学图像分割领域取得了优异的表现，
此后该网络被广泛应用在各个领域，包括DDPM的去噪网络中\cite{DDPM}，该网络是一个典型的编码器-解码器结构，由一个编码器和一个解码器组成，
其中，编码器将输入信号映射到隐空间中，解码器对隐空间中的数据进行解码，进而获得想要的结果。

\begin{figure}[h]
    \centering
    \includegraphics[width=1\textwidth]{Unet.png}
    \caption{NoiseG网络架构}
    \label{figure:Unet}
\end{figure}

该网络以残差卷积块（ResidualBlock）为主要结构（即图中蓝色箭头所表示的运算），
该模块不仅包上一层的输出$x$，还包含了时刻信息$t$，如\cref{table:Unet}所示。

\begin{table}[h]
    \centering
    \caption{Unet中ResidualBlock的结构}
    \label{table:Unet}
    \begin{tblr}{
        colspec = {X[c, 0.5] X[c] X[c] X[c, 2]},
        }
        \toprule
        层数 & 输入 & 网络类型  & 输出\\
        \midrule
        1 & $input$ & GroupNorm & $x_1 = \text{GropuNorm}(input)$, $residue = input$   \\
        2 & $x_1, t$ & SiLU & $x_2 = \text{SiLU}(x_1), t_1 = \text{SiLU}(t)$ \\
        3 & $x_2$ & Conv2d, Linear & $x_3 = \text{Conv2d}(x_2), t_2 = \text{Linear}(t_1)$\\
        4 & $t_2, x_3$ & Sum & $merge_1 = x_3 + t_2$\\
        5 & $merge_1$ & GroupNorm & $merge_2 = \text{GropuNorm}(merge_1$) \\
        6 & $merge_2$ & SiLU & $merge_3 = \text{SiLU}(merge_2)$\\
        7 & $merge_3, residue$ & Conv2d & $merge_4 = \text{Conv2d}(merge_3) + residue$ \\
        8 & $merge_4$ & Output & $Output = merge_4$\\
        \bottomrule
    \end{tblr}
\end{table}
在这些ResidualBlock中，输入的时刻信息$t$为 原始时刻$t_{ori}, t \in \mathbb{R}^1$ 经过TimeEmbedding层线性变换后的代表时刻信息的向量 $t, t\in \mathbb{R}^{1024}$。
经过此变换可以使得时刻信息按照正确的维度添加到网络的隐变量中。TimeEmbedding为一个可学习参数的线性全连接的神经网络（不包含激活函数），用于对时刻作线性变换。
SiLU为激活函数，用于向网络中添加非线性成分。


除此之外，由于原始DDPM中的去噪网络的输入仅为隐变量$\mbs{x}_{t}$以及时刻$t$，EIT问题中电导率分布的生成却需要施加一定的边界条件（即电压-电流信号），
因此适用于EIT图像重建算法的U-net加噪声网络还需要利用边界条件引导噪声的生成，即在网络的输入层添加电压信号的输入。

交叉注意力机制（Cross Attention） 作为一种
注意力机制最常见的变体之一，其通过计算相关性并利用相关性计算加权和的方式，有效地捕捉输入序列之间的相关性，从而很擅长处理如机器翻译等序列到序列的任务。
故本将采用Cross-Attention机制将电压信号嵌入到输入到U-net中，具体而言，通过多个交叉注意力块（Cross Attention Block） 多次将电压信号引入到整个网络的运算中。

本算法所使用到的交叉注意力块（Cross Attention Block）结构如\cref{table:CrossAttention}所示。

\begin{table}[h]
    \centering
    \caption{Cross Attention block}
    \label{table:CrossAttention}
    \begin{tblr}{
        colspec = {X[c, 0.5] X[c] X[c] X[c, 3.2]},
        }
        \toprule
        层数 & 输入 & 网络类型  & 输出\\
        \midrule
        1 & $input$ & Input & $x, v, residue_1 = x$ \\
        2 & $x$ & GroupNorm & $x_1 = \text{GroupNorm}(x)$   \\
        3 & $x_1$ & Conv2d & $x_2 = \text{Conv2d}(x_1), residue_2 = x_2$\\
        4 & $x_2$ & LayerNorm & $x_3 = \text{LayerNorm}(x_2$) \\
        5 & $x_3, residue_2$ & SelfAttention & $x_4 = \text{SelfAttention}(x_3) + residue_2, residue_3 = x_4$ \\
        6 & $x_4$ & LayerNorm & $x_5 = \text{LayerNorm}(x_4)$\\
        7 & $x_5, v, residue_3$ & CrossAttention & $x_6 = \text{CrossAttention}(x_5, v) + residue_3, residue4 = x_6$ \\
        8 & $x_6$ & LayerNorm & $x_7 = \text{LayerNorm}(x_6)$\\
        9 & $x_7, residue_4, residue_1$ & GeLU & $x_8 = \text{GeLU}(x_7) + residue_4 + residue_1$ \\
        \bottomrule
    \end{tblr}
\end{table}

其中，$residue_2, residue_3, residue_4$分别为短期的残差连接，$residue_1$最长的残差连接，用于防止梯度消失，提高网络性能。
Conv2d是卷积核大小为$3\times 3$的卷积层， GroupNorm 和LayerNorm分别是组归一化（Group Norm, GN）和层归一化层，GN将通道划分成若干个组，进而每个组分别进行归一化。GN通常选择固定的组数（本文大量使用到GN，其中归一化组数选择为32），
从而使得归一化的过程不受到batch size的限制，可以有效应用在小批次的样本中。SelfAttention层为自注意力块。
GeLU为激活函数，用于向网络中添加非线性成分。

CrossAttention层则利用Cross Attention机制将电压信号嵌入网络的数据流中，其具体实现方式如下：

\begin{algorithm}[h]
    
    \caption{Cross Attention Layer}
    \begin{algorithmic}[1]
        \State \textbf{Input:} 输入代表电压分布的向量 $\boldsymbol{y}$\\
        以及代表电导率分布的向量（在网络中应为隐变量） $\boldsymbol{X}$。
        \State 初始化权重矩阵 $W_Q$, $W_K$, $W_V$
        \State 计算Q、K、V：$Q = XW_Q, \quad K = yW_K, \quad V = yW_V$
        \State 计算Attention  $A = \text{softmax}\left(\frac{QK^T}{\sqrt{d_k}}\right) \quad $ $d_k$ 为Q和K的维度 
        \State 计算加权后的结果 $Z = AV$
        \State 对Z进行线性变换，获得Cross-Attention最终的输出  $\text{Self-Attention}(X) = ZW_O$
        \State \textbf{Output:} $\text{Cross-Attention}(X, y)$
    \end{algorithmic}
    \label{algorithm:CrossAttention}
\end{algorithm}

其中，与Self-Attention不同的是，Cross-Attention利用电压经线性映射得到矩阵$V, K$，利用电导率分布的隐变量经过线性变换得到矩阵$Q$，
从而求出$K$和$Q$的相关性矩阵，进一步与电压分布引导的矩阵V（value）计算出带权和，将电压分布由带权和的方式嵌入代表电导率分布的隐变量中。

值得一提的是，由于电导率分布为2维矩阵，而电压分布为1维向量，故在进行Cross-Attention之前首先将表示电导率分布的隐变量按照元素从左到右从上到下的顺序映射为1维向量，
然后再将其输入CrossAttentionBlock。
利用CrossAttentionLayer，即可高效地将电压数据嵌入NoiseG中，进而输出重建算法中所需要的特定噪声分布。

由此可得NoiseG的重建过程，如\cref{algorithm:ForwardDiffusion}。
其中，$\text{TimeEmbedding}(t)$是时刻$t$ embedding之后的结果。
$\epsilon_\theta$即为本文所设计的噪声生成网络NoiseG。
\begin{algorithm}[H]
    
    \caption{图像重建过程}
    \begin{algorithmic}[1]

        \State t = T\While{t > 1}
        \State $\mbs{x}_t \sim \mathcal{N}(0, I) \qquad I \in \mathbb{R}^{32 \times 32}$
        \State 利用训练好的网络获得每一步需要减掉的噪声$\epsilon_t$：
        
        $\epsilon_t = \epsilon_\theta(\mbs{x}_t, \text{TimeEmbedding}(t), v)$。
        \State 计算出前一时刻的隐变量$\mbs{x}_{t-1} = \frac{1}{\sqrt{\alpha_t}}(x_t - \frac{\beta_t}{\sqrt{1-\hat{\alpha_t}}}\epsilon_t)$
        \State t = t - 1
        \EndWhile
        \State \textbf{Output:} $x_0$
    \end{algorithmic}
    \label{algorithm:ForwardDiffusion}
\end{algorithm}

其中，$\mbs{x}_t, t$分别为时刻$t$以及$t$时刻的隐变量。$\mbs{v}$是用于引导电导率分布的电压数据。



\xsubsection{图像后处理块}{Postprocessing Block of the Image}

由于图像重建块所重建的电导率分布为编码器编码后的分布，因此为了输入EIT图像，还需要对编码后的数据进行解码。
此部分利用一个解码器对重建结果进行解码，将数据映射到电导率分布空间中。其具体结构如\cref{figure:CCDecoder}所示。
\begin{figure}[h]
    \centering
    \includegraphics[width=1\textwidth]{CCDecoder.png}
    \caption{CDecoder 网络结构}
    \label{figure:CCDecoder}
\end{figure}



该网络中包含了与前文结构相同的ResidualBlock以及SelfAttentionBlock，随后利用GroupNorm层对网络进行归一化，并通过SiLU向其加入非线性部分。
其详细参数如\cref{table:VAE_Decoder}所示。
\begin{table}[H]
    \centering
    \caption{CDecoder解码器架构}
    \label{table:VAE_Decoder}
    \begin{tblr}{
        colspec = {X[c] X[c] X[c] X[c]},
        }
        \toprule
        层数 & 网络类型 & 输入通道数 & 输出通道数 \\
        \midrule
        1 & Conv2d & 64 & 512 \\
        2 & VAE\_SelfAttention &  &  \\
        3 & VAE\_ResidualBlock & 512 & 512 \\
        4 & VAE\_ResidualBlock & 512 & 256 \\
        5 & VAE\_ResidualBlock & 256 & 256 \\
        6 & VAE\_ResidualBlock & 256 & 128 \\
        7 & VAE\_ResidualBlock & 128 & 128 \\
        8 & GroupNorm &  &  \\
        9 & SiLU & & \\
        10 & Conv2d & 64 & 1 \\
        \bottomrule
    \end{tblr}
\end{table}



\xsection{训练和测试数据集的生成策略研究}{Research on Generation of Dataset for Model Training and Testing}

本文所提出的生成式EIT图像重建算法的各部分具体结构均已在上节中作了阐述，该算法共包含了三个完整的神经网络，即：
1） 数据编码块中的电压编码器VEncoder。
2） 图像重建块中用于生成噪声的网络NoiseG。
3） 图像后处理块中的电导率解码器CDecoder。
故本节将分别介绍以上三部分的训练过程。

\subsubsection{VEncoder模型训练}

该部分用于电压分布的编码，使得每组电压-电导率之间具有最高的相关性，该思想首先在CLIP中被使用到\cite{CLIP}，
用来从自然语言监督中学习视觉模型。该模型的训练方法如\cref{figure:Pretrain}所示：

\begin{figure}[H]
    \centering
    \includegraphics[width=.8\textwidth]{Pretrain.PNG}
    \caption{VEncoder训练方式}
    \label{figure:Pretrain}
\end{figure}
该编码器的训练还需要一个用于对原始电导率分布进行编码的编码器CEncoder来辅助。
该方法通过VEncoder和CEncoder分别对电压和电导率数据进行编码，随后将每组编码后的向量按一定次序映射到 $\mathbb{R}^d$空间中共$N$组，其中
$d=1024$是编码器编码后的特征数目，$N=32$是每个batch输入到编码器中的电压-电导率数据的个数。即得到图中的$x_1, x_2,...,x_N$以及
$v_1, v_2,..., v_N$。$x_i,v_i$分别是每一组电压-电导率数据中的电导率分布和电压分布。
随后，对上述两个集合中的元素按照\cref{equation:coor}方式做内积从而计算其余弦相似度，得到相关性矩阵$\mbs{M_{cv}}= \left(m_{i,j}\right)$
则该矩阵中第$(i,j)$个元素表示$x_i$和$v_j$的相关性。
\begin{equation}
    \label{equation:coor}
    m_{i,j} = v_{i} \cdot x_{j}, \qquad i,j=1,2,...,N
\end{equation}


已知该编码器的目标是最大化每组电压-电导率数据内电压和电导率之间的相关性，
故该网络的训练目标即最大化矩阵$\mbs{M_{cv}}$的对角线元素值，同时最小化其他位置的元素值（降低不是同一组的电压-电导率之间的相关性）。
而上述问题可看做N个N分类问题，即对每一个电压向量$v$都应有与其相关性最高的电导率向量$c$。
故可以通过最小化其交叉熵损失来训练该模型，也即\cref{equation:CLIPCrossetp}

\begin{equation}
    loss_c = CrossEntropyLoss \left(\mbs{M_{cv}}, labels, axis=0\right)
\end{equation}
\begin{equation}
    loss_v = CrossEntropyLoss \left(\mbs{M_{cv}}, labels, axis=1\right)
\end{equation}
\begin{equation}
    \label{equation:CLIPCrossetp}
    \min \left(loss_c + loss_v\right)
\end{equation}
其中, $labels$是每个正确分类的标签，分别按照行和列表示对角线元素的下标$1,2,...,N$。

CEncoder的设计如\cref{table:VAE_Conductivity}所示。


\begin{table}[H]
    \centering
    \caption{CEncoder架构}
    \label{table:VAE_Conductivity}
    \begin{tblr}{
        colspec = {X[c] X[c] X[c] X[c]},
        }
        \toprule
        层数 & 网络类型 & 输入通道数 & 输出通道数 \\
        \midrule
        1 & Conv2d & 1 & 128 \\
        2 & VAE\_ResidualBlock & 128 & 128 \\
        3 & VAE\_ResidualBlock & 128 & 256 \\
        4 & VAE\_ResidualBlock & 256 & 256 \\
        5 & VAE\_ResidualBlock & 256 & 512 \\
        6 & VAE\_ResidualBlock & 512 & 512 \\
        7 & VAE\_SelfAttention & 512 & 512 \\
        7 & GroupNorm &  &  \\
        8 & Conv2d & 512 & 64 \\
        \bottomrule
    \end{tblr}
\end{table}

由于EIT电导率分布数据通常具有二维的特征，因此CEncoder利用常见的残差卷积结构作为网络主体，以此来捕获电导率分布的高维特征。

其中，第一层常规的2维卷积，卷积核的大小是$3\times 3$，步长是$1$，填充值为1。
第二层至第6层均为残差卷积块，结构如\cref{table:VAEResidualBlock}所示。在这里引入残差卷积，可以有效解决整个卷积神经网络的优化问题，
并且可以提高网络的训练速度和收敛性，且能降低过拟合的风险。
在CEncoder残差卷积块中，卷积核的大小是$3\times 3$，步长是$1$，填充值为1。并且使用了Group Normalization作为归一化层，能够减少归一化对batch size 的依赖（batch normalization对于batch size的大小非常敏感，其中较小的batch size可能会导致均值和方差不够准确）。
本文所采取的batch size 受计算机性能所限，为32，因此Group Normalization能显著减少网络的内存开销，并且一定程度上提高网络的性能。
此外，该模块中采用了SiLU作为激活函数，相较于一些饱和型激活函数（如Sigmoid和Tanh），可以有效避免在输入较大或较小时导致梯度接近于零，从而避免梯度消失的问题。

\begin{table}[h]
    \centering
    \caption{CEncoder中ResidualBlock的结构}
    \label{table:VAEResidualBlock}
    \begin{tblr}{
        colspec = {X[c, 0.5] X[c] X[c] X[c, 2]},
        }
        \toprule
        层数 & 输入 & 网络类型  & 输出\\
        \midrule
        1 & $input$ & GroupNorm & $x_1 = \text{GropuNorm}(input)$, $residue = input$   \\
        2 & $x_1$ & SiLU & $x_2 = \text{SiLU}(x_1)$ \\
        3 & $x_2$ & Conv2d& $x_3 = \text{Conv2d}(x_2)$\\
        4 & $x_3$ & GroupNorm & $x_4 = \text{GropuNorm}(x_3$) \\
        6 & $x_4$ & SiLU & $x_5 = \text{SiLU}(x_4)$\\
        7 & $x_6, residue$ & Conv2d & $x_7 = \text{Conv2d}(x_6) + residue$ \\
        8 & $x_6$ & Output & $Output = x_6$\\
        \bottomrule
    \end{tblr}
\end{table}




由于该模型的训练需要相应的电压-电导率分布数据对，本文将利用EDIORS（Electrical Impedance Tomography and Diffuse Optical Tomography Reconstruction Software
）仿真根据实际情况生成所需的数据集。
EDIORS 是一个为医学和工业环境中的电阻抗层析成像(EIT)和基于扩散的光学层析成像提供正向和反向建模的免费软件，
该软件可用于EIT数据仿真。

其中，根据\cref{PrincipleEIT}可知，电导率分布的有限元仿真会对待测场域进行三角（或其他方式）剖分，
本文将待测场剖分为2048个三角形单元，如\cref{figure:femg}所示。

\begin{figure}[h]
    \centering
    \includegraphics[width=.5\textwidth]{femg.png}
    \caption{EIT有限元剖分示例}
    \label{figure:femg}
\end{figure}

其中，每两个小三角形单元可以构成一个正方形单元，该有限元模型
通过将每两个小三角形单元的电导率分布值取平均并映射到其拼成的正方形中，可被转化为1024个正方形单元。
整个正方形模型的长和宽均为32个小正方形单元的边长相加而成，其正方形场域$\Omega = \left[-1, 1\right] \times \left[-1, 1\right]$。


采取该方法建立电导率分布的有限元仿真模型，其优点在于，由于矩形单元可代表一个像素点，
故该电导率分布图像可直接又每个矩形的电导率分布映射到最终的EIT图像中，
否则还需要建立从电导率分布到重建结果的线性变换，增加了问题的复杂性。

第二步则是利用上述有限元模型仿真出一组具有不同电导率分布的电压-电导率分布数据对。
为了与物理模型实验的成像目标具有一致的电导率分布，所有仿真数据的背景电导率为0.25S/m，（常温常压下与饱和硫酸钙溶液的电导率相同）。
而扰动目标则是按照扰动目标的半径从小到大分别为$ X = \{0.1, 0.2, 0.3\}$三种尺寸的目标。其电导率分布从$0.1S/m$到$1.0S/m$共有10种不同电导率类型。
按照以上参数生成数据集的具体算法见\cref{algorithm:Dataset}：

\begin{algorithm}[h]
    
    \caption{仿真数据集生成}
    \begin{algorithmic}[1]
        \State 生成背景帧的电导率分布向量
        \State 利用EIT正问题求解器求解出背景帧的边界电压向量
        \State 设置需要仿真的扰动目标的电导率分布集合$X = \{0.1, 0.2, 0.3\}$
        
        以及需要仿真的扰动目标大小的区间以及步长$\sigma \in \left[0, 1\right], step = 0.1$
        \State 设置扰动目标中心需要遍历的待测场域$\Omega = \{ x,y | x^2 + y^2 = 1; x,y \in \left[-1, 1\right]\}$，

        以及遍历的步长$step=0.02$
       \For{扰动目标中心坐标$\left(x, y\right)  \in \Omega$ \quad}
       \For{ $x \in X$}
        \State 将该扰动目标映射为场域内部所对应的剖分单元中
       \If{$(x,y) \in \Omega$}
       \For{ $\sigma = 0.1, 0.2,...,1$}
        \State 生成一个扰动目标所对应的电导率分布向量
        \State 利用EIT正问题求解器求解出其边界电压分部向量
        \State 对生成电压-电导率分布与背景帧作差，获得差分EIT所需的数据对
        \EndFor
        \EndIf
       \EndFor
       \EndFor
       \State 将每一步生成的数据对整合为电压-电导率数据集
    \end{algorithmic}
    \label{algorithm:Dataset}
\end{algorithm}
利用该算法所生成的电压-电导率数据集中的一个数据对如\cref{figure:cond_voltage}所示。

\begin{figure}[h]
    \centering
    \includegraphics[width= .4\textwidth]{cond_voltage.png}
    \caption{生成的电压-电导率数据集示例}
    \label{figure:cond_voltage}
\end{figure}
由于扰动目标大小不同，因此按照上述算法生成的每种类型的电压-电导率数据对的数量不同（扰动目标越大，可遍历的位置空间就越小），
其中，相同大小扰动目标的子数据集具有相同的数据量，见\cref{table:countDataset}。
\begin{table}[h]
  
    
    \caption{不同大小的扰动目标所生成的数据集数量}
    \begin{tblr}{
        colspec = {X[c] X[c]},
    }
    \toprule
    扰动目标的半径 & 数据集的数据量 \\
    \midrule
    0.1 & 6349 \\ 
    0.2 & 5013 \\  
    0.3 & 3841 \\
    \bottomrule
    \end{tblr}
    \label{table:countDataset}
\end{table}


然后将仿真生成的数据集整合为用于训练CEncoder和VEncoder的数据集，
其中数据集的大小为152030，将其按照训练集：测试集 $=$ 0.8来划分训练集和测试集，获得训练集大小121624，测试集大小30406。
即可利用该数据集对数据编码块的训练。经过100次迭代，该网络最终的训练集损失为0.0486，测试集loss为0.006，损失函数随迭代次数的变化如
\cref{figure:CLIPLoss}所示，CEncoder和VEncoder网络在100次迭代后可很好的收敛。
\begin{figure}[h]
    \centering
    \includegraphics[width=.6\textwidth]{CLIPLoss.jpg}
    \caption{数据编码块训练-测试Loss曲线}
    \label{figure:CLIPLoss}
\end{figure}

\subsubsection{NoiseG网络训练}

由于NoiseG的训练需要利用到编码后的电压$v_{encoded}$，代表时刻的向量$t$以及带噪声的隐变量$x_t$生成上一时刻带噪声的隐变量$x_{t-1}$，进而生成电导率分布$\hat{x_{encoded}}$。
其中，该电导率分布为编码后的电导率分布，故还需要编码后的电导率向量$x_{encoded}$与其计算误差。

在上一步训练完成后，我们可以通过训练好的VEncoder和CEncoder，
将原始数据集的电压-电导率对映射到编码后的电压-电导率对，并将此结果作为训练NoiseG的数据集。

NoiseG的训练过程如\cref{algorithm:TrainDDPM}所示，

\begin{algorithm}[h]
    
    \caption{NoiseG的训练}
    \begin{algorithmic}[1]
        \State \textbf{Repate}
        \State 从数据集中选择一个电导率向量：$\mbs{c_0} \sim q(\mbs{c_0})$
        \State 生成一个时刻：$t \sim \mathcal{U}\left[0, T\right]$
        \State 从高斯分布中采样一个噪声：$\mbs{\epsilon} \sim \mathcal{N}\left(\mbs{0}, \mbs{I}\right) $
        \State 根据噪声计算出$t$时刻加噪声后的电导率向量：$x_t = \sqrt{\hat{\alpha_t}}\mbs{c_0} + \sqrt{1-\hat{\alpha_t}}$
        \State 利用NoiseG计算出预测的噪声值：$\mbs{\hat{\epsilon}} = \mbs{\epsilon_{\theta}}\left(x_t\mbs{\epsilon}, t, voltage\right)$
        \State 计算预测噪声与真实噪声的误差并更新网络参数：$\nabla_\theta\left(\mbs{\epsilon} - \mbs{\hat{\epsilon}}\right)^2$
       \State \textbf{uitil} 网络收敛
    \end{algorithmic}
    \label{algorithm:TrainDDPM}
\end{algorithm}

利用该算法，对NoiseG训练所得到的训练集-测试集loss随迭代次数的变化如\cref{figure:DDPMLoss}所示。
由于该模型训练开销过大（单次迭代需要3小时左右）故一共进行了20次迭代至其基本收敛，其最终的训练集loss为0.00384，测试集loss为0.00380。

\begin{figure}[H]
    \centering
    \includegraphics[width=.6\textwidth]{DDPMLoss.PNG}
    \caption{NoiseG训练-测试Loss曲线}
    \label{figure:DDPMLoss}
\end{figure}

此外，由于真实的EIT数据往往包含许多噪声，因此本文还设计了一个针对物理模型实验的网络参数训练方式，如\cref{figure:NoiseGOPT}所示。

\begin{figure}[h]
    \centering
    \includegraphics[width=.95\textwidth]{NoiseGOPT.JPG}
    \caption{针对物理模型的NoiseG训练方式}
    \label{figure:NoiseGOPT}
\end{figure}

该训练方式利用一个噪声生成器（Noise Generator），向不含噪声的电压向量中加入噪声（图中的Noise Generator），并在输出层中添加一个通道的输出作为电压向量的预测值（图中的$v_{pred}$）。
随后最小化如\cref{equation:lossNoiseG}所示的联合损失。
\begin{equation}
    \centering
    \text{Loss} = \|\mbs{v}_{\text{output}} - \mbs{v}_{\text{orignial}}\|^2_2 + \lambda\| \mbs{\epsilon} - \mbs{\epsilon}_\theta\|^2_2
    \label{equation:lossNoiseG}
\end{equation}
该式第一项为预测的电压值与原始（不带噪声）的电压值之间的误差，第二项为原先电导率分布中添加的噪声与其预测值之间的误差。
由此，该训练方式可使得算法对包含噪声的电压数据有更强的鲁棒性。

由于该训练方式需要物理模型实验数据的参与，故其训练过程以及用于生成向电压数据中添加的噪声的生成器设计详见\cref{NoiseRob}。



\subsubsection{CDecoder模型训练}

CDecoder的作用是将编码后的电导率分布尽最大可能还原成原始的电导率分布，故该模型利用CEncoder生成的编码后的电导率分布以及原始的电导率分布作为数据集进行训练。
通过向网络中输入编码后的电导率分布数据，最小化网络输出$\text{CDecoder}(\mbs{c_e})$与原始的电导率分布$\mbs{c}$之间的均方误差，如\cref{equation:CDecodertrain}。

\begin{equation}
    \label{equation:CDecodertrain}
    \min \|\text{CDecoder}(\mbs{c_e}) - \mbs{c}\|_2
\end{equation}

该网络的训练集-测试集loss如\cref{figure:DecoderLoss}所示。
\begin{figure}[h]
    \centering
    \includegraphics[width=.6\textwidth]{DecoderLoss.png}
    \caption{CDecoder训练-测试Loss曲线}
    \label{figure:DecoderLoss}
\end{figure} 

\xsection{模型性能的评估与重建结果分析}{Evaluation of Model Performance and Analysis of Reconstruction Results}

本节将首先介绍EIT图像重建算法常见的评价指标，并利用该指标对上一节中训练好的网络的重建效果进行评估。

\xsubsection{评价指标}{Evaluation Metrics}
\subsubsection{相对误差(Relative Error, RE)}

图像重建的相对误差(RE)是指重建的电导率分布$\mbs{\hat{\sigma}}$与真实电导率$\mbs{\sigma}$之间的误差，是衡量重建图像质量的直观指标，如\cref{equation:RE}。

\begin{equation}
    \label{equation:RE}
    RE = \frac{\| \mbs{\hat{\sigma}} - \mbs{\sigma}\|_2}{\|\mbs{\sigma}\|_2}
\end{equation}

\subsubsection{相关系数(Coorelation Coefficient, CC)}

相关系数(CC)衡量重构电导率分布$\mbs{\hat{\sigma}}$与实际电导率$\mbs{\sigma}$之间的相似度，如\cref{equation:CC}所示。

\begin{equation}
    \label{equation:CC}
    CC = \frac{\sum_{i=1}^{N} \left(\hat{\sigma_i} - \bar{\hat{\sigma}}\right)\left(\sigma_i-\bar{\sigma}\right)}{\sqrt{\sum_{i=1}^{N} \left(\hat{\sigma_i} - \bar{\hat{\sigma}}\right)^2 \sum_{i=1}^{n} \left(\sigma_i-\bar{\sigma}\right)^2}}
\end{equation}

\xsubsection{模型评估}{Model Evaluation}

根据上述评价指标对本文所提出的模型进行评估，如\cref{table:Evaluation}。
\begin{table}[H]
  
    
    \caption{根据RE和CC评估EC-Diffusion 模型}
    \begin{tblr}{
        colspec = {X[c] X[c] X[c]},
    }
    \toprule
    优化器 & 学习率（重建部分） & CC & RE($\times 10^{-5}$) \\
    \midrule
    SGDM & 0.8 & 0.8561 & 9.3354 \\
    SGDM & 0.7 & 0.9017 & 5.9765 \\
    SGDM & 0.5 & 0.9431 & 4.9590 \\
    SGDM & 0.1 & 0.8995 & 8.3535 \\
    SGDM & 0.05 & 0.8736 & 10.7955 \\
    SGDM & 0.01 & 0.8033 & 11.2115 \\
    Adam & 0.01 & 0.9319 & 5.8731 \\
    Adam & 0.005 & 0.9557 & 3.4371 \\
    Adam & 0.0001 & \bf{0.9780} & 2.1073 \\
    Adam & 0.00005 & 0.9718 & \bf{2.0013} \\
    Adam & 0.00001 & 0.9607 & 2.5967 \\
    
    \bottomrule
    \end{tblr}
    \label{table:Evaluation}
\end{table}

\begin{figure}[h]
    \centering
    \includegraphics[width=1\textwidth]{reconsresu.png}
    \caption{重建结果}
    \label{figure:reconsresu}
\end{figure}

根据上述实验选择利用Adam优化器，且学习率为0.0001的条件下的EC-Diffusion为最佳模型
（本文以最小化重建误差为核心目标，故没有选择学利率为0.00005时的模型）。
则部分测试集中的重建结果如\cref{figure:reconsresu}。



其中，Original为仿真数据设置的扰动目标图像，Reconstructed为利用EC-Diffusion算法重建后的EIT图像。可以看出，该算法能有效的重建出高质量的EIT图像，并能很好地区分扰动目标的大小、位置以及电导率分布的情况。


\xsubsection{小结}{Summary}

本章利用编码器-解码器以及改良的扩散概率模型，设计了一个用于脑卒中快速鉴
别的生成式 EIT 图像重建算法。并利用仿真数据训练并验证了网络的有效性。结果表
明，本文所提出的生成式 EIT 图像重建算法能有效地重建出高分辨率的 EIT 图像，为
后续将该算法应用于真实的物理模型数据上提供了基础。






























