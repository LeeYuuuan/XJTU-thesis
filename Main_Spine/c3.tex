% !TeX root = ../main.tex

\xchapter{用于脑卒中快速鉴别的生成式EIT图像重建算法模型}{Model of Generative EIT Reconstruction Algorithm for Fast Detection of Stroke}
传统的EIT图像重建算法通常利用迭代的思想多次求解EIT正问题、逆问题，以不断最小化每一步求解的电压分布和真实电压分布之间的误差。
这种做法通常消耗时间长，成像分辨率低。由于EIT正问题仿真模型完备，且相对精确，而常见的深度学习模型则需要大量的训练数据，因此基于深度学习的EIT图像重建模型大大提高了EIT图像重建算法的重建分辨率，
并且由于训练好的深度学习模型正向推理的过程通常耗时较少，因而能显著提高图像重建算法的时间效率。这也为EIT技术实现床旁实时监测、院前疾病监测提供了技术保障。

目前主流的利用深度学习技术实现的EIT图像重建算法通常以卷积神经网络的各种变体为模型基础，通过添加具有不同功能的模块来优化网络性能。
此类方法通常使用判别式模型来实现。

扩散概率模型，作为生成式模型的一种，近年来在图像生成领域已经取得了显著的成果。而EIT图像重建问题本质上是建立从电压分布到电导率分布的非线性映射，
即可以看作是生成目标为电导率分布，且指导模型生成电导率分布的条件为电压分布的带条件的扩散概率模型。
利用扩散模型实现EIT图像重建算法，由于扩散模型加噪声、去噪声的过程而一定程度上提高了模型抗噪声的能力，并且通过对推断过程的改进，可以有效减少在推理过程中的采样次数，极大程度的缩短了模型正向计算的过程，进而提高了EIT图像重建效率。

本章将重点介绍用于脑卒中快速鉴别的生成式EIT图像重建算法的模型架构。包括模型总体的结构、模型各部分的结构和作用、模型训练集的设计和仿真、训练过程、训练结果以及对模型性能的评估和优化。

\xsection{用于脑卒中快速鉴别的生成式EIT图像重建算法模型架构}{Architecture of the Model of Generative EIT Reconstruction Algorithm for Fast Detection of Stroke}

\xsubsection{模型的总体架构}{The Overall Architecture of The Model}
\label{section:Model}
由于EIT图像重建任务中，不同类型的数据维度不同，同一类型的数据也包含多种表示方式，因此本节将首先建立用于脑卒中快速鉴别的生成式EIT图像重建算法模型，然后再对其各个部分进行逐一阐述。

设待测场$\Omega$所包含的剖分单元个数为$d_e$，边界为$\partial \Omega$。其边界的电极个数为$d_v$。
EIT采集系统的激励模式为邻近激励，单次测量所有激励模式所构成的集合为$S_{stim}$，单次测量电流激励的次数为$n_m$；测量电压的模式为邻近差分，单次激励测量电压的维度为$d_{vm}$。
则单次测量的电压向量维度为$d_m = d_{vm} * n_m$。（关于EIT的有限元模型和测量模式详见）。

则EIT图像重建的电导率分布向量为 $\sigma$，其维度为 $1 \times d_e$；EIT的测量电压可分别表为矩阵形式$v_{mtx} = \{v_{ij}\}, i \in S_{stim}, j =1,2,...d_vm$。其中$v_{ij}$表示以$i$为激励方式的第$j$次测量的结果。
向量形式$v_{vector}$，其维度为$d_m$。EIT重建的图像分辨率为$res_{row} \times res_{column}$。

如无特殊说明，本文所采用的参数为\cref{table:param_stm}中的默认值。
\begin{table}
    \centering
    
    \caption{参数设置}
    \begin{tblr}{hlines,
        vlines,
        colspec = {X[c] X[c] X[c] X[c]},
    }
 
    参数 & 值(域) & 参数 & 值(域) \\
    \midrule
    $\Omega$ & 圆形场域 $ x \in [-1, 1], y \in[-1, 1]$ & $S_{stim}$ & [i, i+1], i=(0:15)\%15 \\
    $d_e$ & 1024 & $n_m$ & 16 \\
    $d_v$ & 16 & $d_{vm} $ & 16 \\
    $d_m$ & 256 & $\sigma_i$ & $[1, d_e]$ \\
    $res_{row}$ & 16 & $res_{column}$ & 16 \\

 
    \end{tblr}
    \label{table:param_stm}
\end{table}


如图所示，本文提出的用于脑卒中快速鉴别的生成式EIT图像重建算法模型共包括三个模块，分别是：
\begin{enumerate}
    \item 数据编码块。用于对电压数据、电导率数据进行编码，以输出适应图像重建模块的输入维度。此外，根据测试结果表明，该部分还能提高对应电压-电导率数据之间的相关性，进一步优化网络性能（见）。
    \item 图像重建块。该部分利用改进的扩散概率模型拟合EIT图像重建映射，输出编码后的电导率分布。该模块是本算法的核心部分。
    \item 图像后处理块。用于解码图像重建块的生成结果，将生成结果映射到用于引导图像的电导率分布所适应的维度。
\end{enumerate}
以下将对三个模块分别详细解析。
\xsubsection{数据编码块}{Data Encoding Block}
此模块由两个编码器构成，分别对电压和电导率分布进行编码。由于电压向量的表示方式不同，而神经网络对于输入向量的维度敏感度极高，因此此部分首先将输入的电压向量（或矩阵）映射为一维行向量，其长度为$d_m$。

电导率编码器（CEncoder）、电压编码器（VEncoder）分别为两个编码器，用于对电导率分布向量和电压分布向量进行编码。
其中，CEncoder的结如图所示，该部分的具体参数如\cref{table:VAE_Conductivity}所示。


\begin{table}[H]
    \centering
    \caption{CEncoder架构}
    \label{table:VAE_Conductivity}
    \begin{tblr}{
        colspec = {X[c] X[c] X[c] X[c]},
        }
        \toprule
        层数 & 网络类型 & 输入通道数 & 输出通道数 \\
        \midrule
        1 & Conv2d & 1 & 128 \\
        2 & VAE\_ResidualBlock & 128 & 128 \\
        3 & VAE\_ResidualBlock & 128 & 256 \\
        4 & VAE\_ResidualBlock & 256 & 256 \\
        5 & VAE\_ResidualBlock & 256 & 512 \\
        6 & VAE\_ResidualBlock & 512 & 512 \\
        7 & VAE\_SelfAttention & 512 & 512 \\
        7 & GroupNorm &  &  \\
        8 & Conv2d & 512 & 64 \\
        \bottomrule
    \end{tblr}
\end{table}

由于EIT电导率分布数据通常具有二维的特征，因此CEecoder利用常见的残差卷积结构作为网络主体，以此来捕获电导率分布的高维特征。

其中，第一层常规的2维卷积，卷积核的大小是$3\times 3$，步长是$1$，填充值为1。
第二层至第6成均为残差卷积块，结构如\cref{table:VAEResidualBlock}所示。在这里引入残差卷积，可以有效解决整个卷积神经网络的优化问题，
并且可以提高网络的训练速度和收敛性，且能降低过拟合的风险。
在CEncoder残差卷积块中，卷积核的大小是$3\times 3$，步长是$1$，填充值为1。并且使用了Group Normalization作为归一化层，能够减少归一化对batch size 的依赖（batch normalization对于batch size的大小非常敏感，其中较小的batch size可能会导致均值和方差不够准确）。
本文所采取的batch size 受计算机性能所限，为32，因此Group Normalization能显著减少网络的内存开销，并且一定程度上提高网络的性能。
此外，该模块中采用了SiLU作为激活函数，相较于一些饱和型激活函数（如Sigmoid和Tanh），可以有效避免在输入较大或较小时导致梯度接近于零，从而避免梯度消失的问题。

\begin{table}
    \centering
    \caption{CEncoder残差卷积块}
    \label{table:VAEResidualBlock}
    \begin{tblr}{colspec = {X[c] X[c] X[c] X[c]}}
    \toprule
    层数 & 网络类型 & 输入通道数 & 输出通道数 \\
    \midrule
    1 & residule & & \\
    2 & GroupNorm & & \\
    3 & SiLU & &\\
    4 & 2dConv  & in\_channels &   out\_channels \\
    5 & GroupNorm & & \\
    6 & SiLU & & \\
    7 & 2dConv &  out\_channels &  out\_channels  \\
    8 & Conv2d(residule) + outputlayer7 & & \\
    \bottomrule
    \end{tblr}
\end{table}


    值得注意的是，其第七层为SelfAttention层，该层由一个LayerNorm层 和一个SelfAttention模块和组合而成。
    
    Self-attention层是注意力机制的一种应用，其具体的计算过程如\cref{algorithm:Self_attention}。该机制允许模型在处理电导率分布时专注于数据的某些部分（如扰动目标的部分），而忽略其他部分（即背景帧）。
    该模块可以使得电导率分布在不同的位置之间保持联系，以实现对扰动部分更好地成像。
    
\begin{algorithm}[H]
    
    \caption{Self Attention Layer}
    \begin{algorithmic}[1]
        \State \textbf{Input:} 输入向量 $\boldsymbol{X}$代表网络中的隐变量。
        \State 初始化权重矩阵：$W_Q$, $W_K$, $W_V$
        \State 计算Q、K、V：$Q = XW_Q, \quad K = XW_K, \quad V = XW_V$
        \State 计算Attention：$A = \text{softmax}\left(\frac{QK^T}{\sqrt{d_k}}\right) \quad $ $d_k$ 为Q和K的维度 
        \State 计算加权后的结果：$Z = AV$
        \State 对Z进行线性变换，获得Self-Attention最终的输出 ：$\text{Self-Attention}(X) = ZW_O$
        \State \textbf{Output:} $\text{Self-Attention}(X)$
    \end{algorithmic}
    \label{algorithm:Self_attention}
\end{algorithm}

    具体而言， 由于EIT图像重建任务中，其电导率分布按照扰动目标的存在与否通常可被分为两个部分：背景帧溶液和扰动目标部分。
    其中，背景帧部分通常模拟电导率分布相对固定的电解质溶液，用于代表脑脊液的区域。这部分通常对于EIT成像结果影响较小（即应该降低该部分对于EIT图像重建结果的影响），因此需要降低模型对于该部分的关注度。
    而扰动目标区域则用于模拟特定的病灶区域（由于这些区域的含水量与背景帧不同而导致的电导率变化）。这部分通常对于EIT成像结果影响显著（即扩大该部分数据对于EIT重建结果的影响）。
    而Self Attention机制通过计算一组数据内部的相关性并对每个元素重新赋上加权和，可以有效的关注其输入数据的特定部分。因此在此处引入Self Attention层将一定程度上提高扰动目标区域对电压分布或电导率分布中相关部分的关注程度，
    进而可以有效地提高网络对EIT非线性映射的拟合能力。


\begin{table}[H]
    \centering
    \caption{VEncoder编码器架构}
    \label{table:VAE_Voltage}
    \begin{tblr}{
        colspec = {X[c] X[c] X[c]},
    }
    \toprule
    层数 & 网络类型 \\
    \midrule
    1 & Linear  \\
    2 & residue 1 \\
    3 & LayerNorm \\
    4 & SelfAttention  + residue 1\\
    5 & residue 2 \\
    6 & LayerNorm \\
    7 & Linear \\
    8 & QuickGELU \\
    10 & RBFNN \\
    11 & Linear  + residue 2\\ 
    \bottomrule
    \end{tblr}
\end{table}
VEecoder用于编码边界电压分布，因此要求此编码器能实现对电压分布的深层次特征进行提取，更好地捕获其特征与电导率分布之间的关系。
此部分的网络结构如\cref{table:VAE_Voltage}所示。关于RBF网络性能的验证见\cref{RBF}






\xsubsection{图像重建块}{Image Reconstruction Block}
此模块利用改良的扩散概率模型实现了EIT图像重建算法，如\cref{figure:ddpm_EIT}所示。

\begin{figure}[h]
    \centering
    \includegraphics[width=.8\textwidth]{ddpm_EIT.JPG}
    \caption{利用扩散概率模型实现EIT图像重建}
    \label{figure:ddpm_EIT}
\end{figure}

其中，通过不断添加高斯噪声使得电导率分布映射到到各向同性的高斯分布的过程称为扩散过程。在该过程中，初始经过编码后的电导率分布将随着时间逐渐扩散，并且不断接近高斯分布。
而将高斯分布通过加权的方式去噪声的过程通常称为逆扩散过程。模型结构如\cref{figure:DDPMreconstruction}所示。


\begin{figure}[h]
    \centering
    \includegraphics[width=.9\textwidth]{DDPMreconstruction.PNG}
    \caption{图像重建块模型结构}
    \label{figure:DDPMreconstruction}
\end{figure}

其中，Sampler 为用于采样 $t$时刻加噪声后的数据$x_t$，即Diffusion model的正向扩散过程；
Denoising Net 用于生成每一步需要去掉的噪声分布。该部分需要根据输入的当前时间$t$、引导图像生成的条件向量（本文中为测量的边界电压分布）$v$以及当前时刻生成结果$x_t$，
来预测$t-1$时刻需要减掉的噪声，进一步计算出$x_{t-1}$步的生成结果。不断循环预测的过程，即可回复原始数据$x_0$。
本节将首先介绍整个图像重建块的架构，然后再阐述本文所实现的生成式EIT图像重建算法中生成噪声的网络的架构以及如何利用Cross-Attention机制将电压分布嵌入到网络的输入中。

\subsubsection{图像重建块实现}

图像重建块在推理阶段时，首先随机生成一个大小为$32\times 32$的高斯噪声（与电导率分布编码后的大小相同）作为$t$时刻的隐变量$x_t$，
然后通过向训练好的生成噪声网络$\epsilon_\theta$输入该时刻的隐变量$x_t$、当前时刻$t$以及经过上一节中电压编码器VEncoder编码后的电压向量$v$，
输入上一时刻的隐变量$x_{t-1}$，经过不断迭代最终输出该电压分布作为边界条件所对应的电导率分布的编码后的结果。其算法流程如\cref{algorithm:ForwardDiffusion}

\begin{algorithm}[H]
    
    \caption{图像重建过程}
    \begin{algorithmic}[1]
        \State t = T
        \While{t > 1}
        \State 从高斯分布中采样一个随机噪声作为 $t$时刻的隐变量
        
        $x_t \sim \mathcal{N}(0, I) \qquad I \in \mathbb{R}^{32 \times 32}$
        \State 利用训练好的网络获得每一步需要减掉的噪声$\epsilon_t$：
        
        $\epsilon_t = \epsilon_\theta(x_t, TimeEmbedding(t), v)$。
        \State 计算出前一时刻的隐变量$x_{t-1} = \frac{1}{\sqrt{\alpha_t}}(x_t - \frac{\beta_t}{\sqrt{1-\hat{\alpha_t}}}\epsilon_t)$
        \State t = t - 1
        \EndWhile
        \State \textbf{Output:} $x_0$
    \end{algorithmic}
    \label{algorithm:ForwardDiffusion}
\end{algorithm}

\subsubsection{生成噪声网络的架构}

扩散模型中最主要的结构即是用于生成噪声的网络$\epsilon_\theta$，本文将采用改良后的U-net作为生成噪声的网络，
U-net是由Olaf Ronneberger等人于2015年提出的基于CNN的深度学习模型\cite{2015U}，该网络在医学图像分割领域取得了优异的表现，
此后该网络被广泛应用在各个领域，包括DDPM的去噪网络中\cite{DDPM}，该网络是一个典型的编码器-解码器结构，由一个编码器和一个解码器组成，
其中，编码器将输入信号映射到隐空间中，解码器对隐空间中的数据进行解码，进而获得想要的结果。
因此，本文同样采用U-net作为该网络的基本架构，如\cref{figure:Unet}所示。

\begin{figure}
    \centering
    \includegraphics[width=0.8\textwidth]{Unet.png}
    \caption{U-net网络架构}
    \label{figure:Unet}
\end{figure}

其中，除了CrossAttention层以外，该网络还包含多个ResidualBlock，如图中的Conv3$\times$3, SiLU, TimeEmbedding所示。
该模块不仅包上一层的输出$x$，还包含了时刻信息$t$，且同样利用了残差连接的方式，其具体的结构如\cref{table:Unet}所示。

\begin{table}[H]
    \centering
    \caption{Unet中ResidualBlock的结构}
    \label{table:Unet}
    \begin{tblr}{
        colspec = {X[c] X[c]},
        }
        \toprule
        层数 & 网络类型（作用对象） & \\
        \midrule
        1 & residue = residue(x) \\
        2 & x = GroupNorm(x) \\
        3 & x = SiLU(x) \\
        4 & x = conv2d(x) \\
        5 & t = SiLU(t) \\
        6 & t = Linear(t) \\
        7 & merge = x + t \\
        8 & merge = GroupNorm(merge) \\
        9 & merge = SiLU(merge) \\
        10 & merge = Conv2d(merge) \\
        11 & merge = merge + residue \\
        \bottomrule
    \end{tblr}
\end{table}

在这些ResidualBlock中，输入的时刻信息$t$为 原始时刻$t_{ori}, t \in \mathbb{R}^1$ 经过TimeEmbedding层线性变换后的代表时刻信息的向量 $t, t\in \mathbb{R}^1024$。
经过此变换可以使得时刻信息按照正确的维度添加到网络的隐变量中。TimeEmbedding为一个可学习参数的线性全连接的神经网络（不包含激活函数），用于对时刻作线性变换。
该模块中的其他部分均与上文中的意义相同。




\subsubsection{交叉注意力机制层}{Cross Attention Layer}

由于原始DDPM中的去噪网络的输入仅为隐变量$x_t$以及时刻$t$，EIT问题中电导率分布的生成却需要施加一定的边界条件（即电压-电流信号），
因此适用于EIT图像重建算法的U-net加噪声网络还需要利用边界条件引导噪声的生成，即在网络的输入层添加电压信号的输入。交叉注意力机制（Cross Attention） 作为一种
注意力机制最常见的变体之一，其利用计算相关性并利用相关性计算加权和的方式，有效地捕捉输入序列之间的相关性，从而很擅长处理如机器翻译等序列到序列的任务。
故本将采用Cross-Attention机制将电压信号嵌入到输入到U-net中，具体而言，通过多个交叉注意力块（cross Attention block） 多次将电压信号引入到整个网络对电导率分布的拟合过程中。

本算法所使用到的U-net 中包含了一个Cross Attention layer，其具体结构如\cref{table:CrossAttention}所示。

\begin{table}[H]
    \centering
    \caption{cross-Attention block}
    \label{table:CrossAttention}
    \begin{tblr}{
        colspec = {X[c] X[c] X[c]},
    }
    \toprule
    层数 & 网络类型（作用目标） \\
    \midrule
    1 & residuelong = x \\
    2 & GroupNorm(x) \\
    3 & Conv2d(x) \\
    4 & residue2 = x\\
    5 & LayerNorm(x) \\
    6 & SelfAttention(x) \\
    7 & x + residue2 \\
    8 & residue3 = x\\
    9 & LayerNorm(x)\\
    10 & CrossAttention(x, v)\\
    11 & x + residue3\\
    12 & residue4 = x\\
    13 & LayerNorm(x) \\
    14 & GeLU(x) \\
    15 & x + residue4 + residuelong\\
    \bottomrule
    \end{tblr}
\end{table}

其中，residue1，residue2，residue3分别为短期的残差连接，residuelong最长的残差连接，用于防止梯度消失，提高网络性能。
Conv2d是卷积核大小为$3\times 3$的卷积层， GroupNorm 和LayerNorm分别是组归一化和层归一化层， SelfAttention层为自注意力块，计算方式见\cref{algorithm:Self_attention}。
GeLu为激活函数，用使网络具有非线性成分。

CrossAttention层则利用Cross Attention机制将电压信号嵌入网络的数据流中，其具体实现方式如下：

\begin{algorithm}[H]
    
    \caption{Cross Attention Layer}
    \begin{algorithmic}[1]
        \State \textbf{Input:} 输入代表电压分布的向量 $\boldsymbol{y}$\\
        以及代表电导率分布的向量（在网络中应为隐变量） $\boldsymbol{X}$。
        \State 初始化权重矩阵 $W_Q$, $W_K$, $W_V$
        \State 计算Q、K、V：$Q = XW_Q, \quad K = yW_K, \quad V = yW_V$
        \State 计算Attention  $A = \text{softmax}\left(\frac{QK^T}{\sqrt{d_k}}\right) \quad $ $d_k$ 为Q和K的维度 
        \State 计算加权后的结果 $Z = AV$
        \State 对Z进行线性变换，获得Cross-Attention最终的输出  $\text{Self-Attention}(X) = ZW_O$
        \State \textbf{Output:} $\text{Cross-Attention}(X, y)$
    \end{algorithmic}
    \label{algorithm:Self_attention}
\end{algorithm}

其中，与Self-Attention不同的是，Cross-Attention利用电压经线性映射得到矩阵$V, K$，利用电导率分布的隐变量经过线性变换得到矩阵$Q$，
从而利用求出$K$和$Q$的相关性矩阵，进一步与电压分布引导的矩阵V（value）计算出带权和，将电压分布由带权和的方式嵌入代表电导率分布的隐变量中。
值得一提的是，由于电导率分布为2维矩阵，而电压分布为1维向量，故在进行Cross-Attention之前首先将电压分布按照元素从左到右从上到下的顺序映射为1维向量，
然后再将其输入CrossAttentionBlock。
利用CrossAttentionLayer，即可高效地将电压数据嵌入生成噪声的网络中，进而输出重建算法中所需要的特定噪声分布。



\xsubsection{图像后处理块}{Postprocessing Block of the Image}
由于图像重建块所重建的电导率分布为编码器编码后的分布，因此为了输入EIT图像，还需要对编码后的数据进行解码。
此部分利用一个解码器对重建结果进行解码，将数据映射到电导率分布空间中。其具体结构如\cref{table:VAE_Decoder}所示。
其中残差卷积块和SelfAttention块均与编码器一致。
\begin{table}[H]
    \centering
    \caption{CDecoder解码器架构}
    \label{table:VAE_Decoder}
    \begin{tblr}{
        colspec = {X[c] X[c] X[c] X[c]},
        }
        \toprule
        层数 & 网络类型 & 输入通道数 & 输出通道数 \\
        \midrule
        1 & Conv2d & 64 & 512 \\
        2 & VAE\_SelfAttention &  &  \\
        3 & VAE\_ResidualBlock & 512 & 512 \\
        4 & VAE\_ResidualBlock & 512 & 256 \\
        5 & VAE\_ResidualBlock & 256 & 256 \\
        6 & VAE\_ResidualBlock & 256 & 128 \\
        7 & VAE\_ResidualBlock & 128 & 128 \\
        8 & GroupNorm &  &  \\
        9 & SiLU & & \\
        10 & Conv2d & 64 & 1 \\
        \bottomrule
    \end{tblr}
\end{table}





\xsection{径向基函数神经网络}{Radial Basis Function Neural Network}
\label{RBF}

此节计了一个改良的卷积神经网络结构，用于以验证该该RBF对于EIT电压-电导率分布映射强大的拟合能力。
为验证RBF神经网络对于EIT数据优秀的拟合能力，首先利用仿真模型获得EIT电压-电导率分布数据集，
其中，$d_m =  576$ ，即剖分的单元个数为576。$d_v = 192$ 即测量电压向量为192维。背景电导率为$0.15S/m$，扰动目标的中心电导率为0.7S/m。
大小为六边形，且其电导率根据距中心的距离而减小，如\cref{figure:RBF_cond}所示。
\begin{figure}[h]
    \centering
    \includegraphics[width=.5\textwidth]{RBF_cond.png}
    \caption{EIT仿真结果}
    \label{figure:RBF_cond}
\end{figure}

由于卷积层输出的每个元素都包含原始数据的空间特征，因此输出层包含 512 个神经元，代表更深层次的特征。
此后为一个径向基函数神经网络（RBFNN），输入层有512个神经元，输出层有576个神经元，分别代表每个元素的电导率。
众所周知，多层感知器包含点积（输入和权重之间）和激活（非线性）函数（例如 ReLU、Sigmoid 函数）。
网络训练通常是通过所有层的反向传播来完成的。
与 MLP 不同，RBFNN 使用欧几里德距离（输入和权重之间，也称为中心）和高斯激活函数，这使得神经元更加局部敏感。 
RBF 网络具有与 MLP 类似的输入层和输出层。
不同的是，隐藏层中的每个神经元都有一个原型向量和一个由 $\mu$ 和 $sigma$ 表示的带宽分别计算输入向量与其原型向量之间的相似度，如\cref{equation:RBFNN}所示。
\begin{equation}
    \label{equation:RBFNN}
    y(x) = f(z(x)) = \sum_{i=1}^{m} v_i \phi_i(z(x)) + b
\end{equation}
其中：
\begin{equation}
    \phi_i(z(x)) = \exp\left(-\frac{|x - \mu_i|^2}{2\sigma_i^2}\right)
\end{equation}

\begin{itemize}
    \item $y(x)$ 是 RBFNN 的输出；
    \item $f(\cdot)$ 是输出层的激活函数；
    \item $z(x)$ 是隐含层的输出；
    \item $\phi_i(z(x))$ 是隐含层神经元 $i$ 的激活函数；
    \item $v_i$ 是连接隐含层和输出层的权重；
    \item $b$ 是偏置项；
    \item $m$ 是隐含层神经元的数量。
\end{itemize}



该网络首先包含 192 个神经元的输入层，分别对应于边界电压的每个维度。接下来有 6 个卷积层，每一步应用 3 × 3 卷积核和最大池化。最后两层应用 3×3 卷积核和 2×2 卷积核平均池，在每个卷积层之后使用指数线性单元（ELU）作为激活函数，因为 ELU 是。
由于卷积层输出的每个元素都包含原始数据的空间特征，因此输出层包含 512 个神经元，代表更深层次的特征。

利用训练数据初步重建模型，所得的结果如\cref{figure:recons_rbf}所示，其中，
第一行中的第一张图像是真实值。另外两个分别采用NR方法和学习率=0.0005模型的CNN + RBFNN（Adam）重建。
第二行的前两张图像由 CNN+ RBFNN (Adam) 重建，学习率 = 0.0001 和 0.001。第三个则则是由单独的CNN（Adam）重建。
\begin{figure}[H]
    \centering
    
    \includegraphics[width=.7\textwidth]{recons_rbf.png}
    \label{figure:recons_rbf}
    \caption{重建结果对比}
\end{figure}

\cref{table:RBFNNev}为对该网络性能的评估，其中RE和CC分别为相对误差(relative error)和 相关系数(correlation coefficient)。
(表中仅为最佳学习率下的性能其中CNN)
\begin{table}[H]
    \centering
    \caption{网络评估}
    \label{table:RBFNNev}
    \begin{tblr}{colspec={X[c] X[c] X[c] X[c]}}
        \toprule
        模型类型 & 学习率 & CC & RE($\times 10^{-5}$) \\
        \midrule
        CNN+RBF(Adam) & 0.0005 & 0.9595 & 5.0552 \\ 
        CNN+RBF(SGDM) & 0.8& 0.9020 & 7.2939 \\
        CNN(Adam) & 0.0005 & 0.3694 & 15.0147 \\
        \bottomrule
    \end{tblr}
\end{table}

通过相对误差和重建结果可分析得出，带有RBFNN的重建结果相较于不带RBFNN的重建结果有显著提升，
因此认为RBFNN对于EIT电压-电导率映射的拟合有显著作用。
此外，adam优化器对EIT重建结果具有显著的提升。


\xsection{训练和测试数据集的设计与生成}{Design and Generation of the Dataset for Model Training and Testing}

本文所提出的生成式EIT图像重建算法的各部分具体结构均已在上节中作了阐述，该算法共包含了四个完整的神经网络，即
\begin{enumerate}
    \item 数据编码块中的电压编码器VEncoder。
    \item 数据编码块中的电导率编码器CEncoder。
    \item 图像重建块中用于生成噪声的网络，为方便起见称其为噪声生成器NoiseG。
    \item 图像后处理块中的电导率解码器CDecoder。
\end{enumerate}
故本节将重点介绍以上四部分的训练过程。

\subsubsection{CEncoder和VEncoder模型训练}
该部分用于将原始的电压和电导率分布编码，使得每组电压-电导率之间具有最高的相关性，该思想首先在CLIP中被使用到\cite{CLIP}，
用来从自然语言监督中学习视觉模型。该模型的训练方法如\cref{figure:Pretrain}所示：

\begin{figure}[H]
    \centering
    \includegraphics[width=.8\textwidth]{Pretrain.PNG}
    \caption{数据编码块架构}
    \label{figure:Pretrain}
\end{figure}

即通过CEncoder和VEncoder分别对电压和电导率数据进行编码，随后将每组编码后的向量按按一定次序映射到 $\mathbb{R}^d$空间中共$N$组，其中
$d=1024$是编码器编码后的特征数目，$N=32$是每个batch输入到编码器中的电压-电导率数据的个数。即得到图中的$x_1, x_2,...,x_N$以及
$v_1, v_2,..., v_N$。$x_i,v_i$分别是每一组电压-电导率数据中的电导率分布和电压分布。
随后，对上述两个集合中的元素按照\cref{equation:coor}方式做内积从而计算其余弦相似度，得到相关性矩阵$\mbs{M_{cv}}= \left(m_{i,j}\right)$
则该矩阵中第$(i,j)$个元素表示$x_i$和$v_j$的相关性。
\begin{equation}
    \label{equation:coor}
    m_{i,j} = v_{i} \cdot x_{j}, \qquad i,j=1,2,...,N
\end{equation}


已知该编码器的的目标是最大化每组电压-电导率数据内电压和电导率之间的相关性，
故该网络的训练目标即最大化矩阵$\mbs{M_{cv}}$的对角线元素值，同时最小化其他位置的元素值（降低不是同一组的电压-电导率之间的相关性）。
而上述问题可看做N个N分类问题，即对每一个电压向量$v$都应有与其相关性最高的电导率向量$c$。
故可以通过最小化其交叉熵损失来训练该模型，也即\cref{equation:CLIPCrossetp}

\begin{equation}
    loss_c = CrossEntropyLoss \left(\mbs{M_{cv}}, labels, axis=0\right)
\end{equation}
\begin{equation}
    loss_v = CrossEntropyLoss \left(\mbs{M_{cv}}, labels, axis=1\right)
\end{equation}
\begin{equation}
    \label{equation:CLIPCrossetp}
    \min \left(loss_c + loss_v\right)
\end{equation}
其中, $labels$每个正确分类的标签，分别按照行和列表示对角线元素的下标$1,2,...,N$。

由于该模型的训练需要相应的电压-电导率分布数据对，本文将利用EDIORS仿真根据实际情况生成所需的数据集。
其中，根据\cref{PrincipleEIT}可知，电导率分布的有限元仿真会对待测场域进行三角（或其他方式）剖分，
本文将待测场剖分为2048个三角形单元，如图所示。其中，每两个小单单元可以构成一个正方形单元，该有限元模型
通过将每两个小三角形单元的电导率分布值取平均并映射到其拼成的正方形中，可被转化为1024个正方形单元。
整个正方形模型的长和宽均为32个小正方形单元的边长相加而成。

采取该方法建立电导率分布的有限元仿真模型，其优点在于，由于矩形单元可代表一个像素点，
故该电导率分布图像可直接又每个矩形的电导率分布映射到最终的EIT图像中，
否则还需要建立从电导率分布到重建结果的线性变换，增加了问题的复杂性。

--电导率分布怎么放进去
--电压数据的维度和差分
--共包含多少？


如\cref{figure:encoding_1}所示。



\begin{figure}[h]
    \centering
    \includegraphics[width=.5\textwidth]{encoding_1.png}
    \caption{编码后的电导率和真实电导率对比}
    \label{figure:encoding_1}
\end{figure}

\begin{figure}[h]
    \centering
    \includegraphics[width=.5\textwidth]{cond_voltage.png}
    \caption{编码后的电导率和真实电导率对比}
    \label{figure:cond_voltage}
\end{figure}

\xsection{模型性能的评估和优化}{Evaluation and Optimization of Model Performance}
